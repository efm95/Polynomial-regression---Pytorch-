{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda import random\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "\n",
    "import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_st        = torch.Tensor([-8.,-4.,2.,1.])\n",
    "x_rang      = torch.Tensor([-3.,2.])\n",
    "sig         = 0.5\n",
    "num_samples = 100\n",
    "seed_train  = 0\n",
    "seed_val    = 1\n",
    "\n",
    "X_train, y_train = regression.create_dataset(w_star=w_st,x_range=x_rang,sample_size=num_samples,sigma=sig,seed=seed_train)\n",
    "X_val, y_val = regression.create_dataset(w_star=w_st,x_range=x_rang,sample_size=num_samples,sigma=sig,seed=seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_train[:,1],y_train,'.',label= 'Training')\n",
    "plt.plot(X_val[:,1],y_val,'.',label= 'Test')\n",
    "plt.legend()\n",
    "plt.title('Generated data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: train loss: 33.25267028808594\n",
      "Step 0: val loss: 31.347246170043945\n",
      "Step 1: train loss: 31.3008975982666\n",
      "Step 1: val loss: 29.882726669311523\n",
      "Step 2: train loss: 29.796567916870117\n",
      "Step 2: val loss: 28.477405548095703\n",
      "Step 3: train loss: 28.598388671875\n",
      "Step 3: val loss: 27.48324966430664\n",
      "Step 4: train loss: 27.6136531829834\n",
      "Step 4: val loss: 26.514341354370117\n",
      "Step 5: train loss: 26.780977249145508\n",
      "Step 5: val loss: 25.762697219848633\n",
      "Step 6: train loss: 26.059297561645508\n",
      "Step 6: val loss: 25.030893325805664\n",
      "Step 7: train loss: 25.420732498168945\n",
      "Step 7: val loss: 24.42031478881836\n",
      "Step 8: train loss: 24.8460636138916\n",
      "Step 8: val loss: 23.829980850219727\n",
      "Step 9: train loss: 24.321760177612305\n",
      "Step 9: val loss: 23.31151580810547\n",
      "Step 10: train loss: 23.83810043334961\n",
      "Step 10: val loss: 22.813404083251953\n",
      "Step 11: train loss: 23.38794708251953\n",
      "Step 11: val loss: 22.360715866088867\n",
      "Step 12: train loss: 22.96588897705078\n",
      "Step 12: val loss: 21.927274703979492\n",
      "Step 13: train loss: 22.567781448364258\n",
      "Step 13: val loss: 21.524425506591797\n",
      "Step 14: train loss: 22.19033432006836\n",
      "Step 14: val loss: 21.138904571533203\n",
      "Step 15: train loss: 21.830904006958008\n",
      "Step 15: val loss: 20.775197982788086\n",
      "Step 16: train loss: 21.487323760986328\n",
      "Step 16: val loss: 20.42661476135254\n",
      "Step 17: train loss: 21.157791137695312\n",
      "Step 17: val loss: 20.094377517700195\n",
      "Step 18: train loss: 20.84079360961914\n",
      "Step 18: val loss: 19.77509307861328\n",
      "Step 19: train loss: 20.53504180908203\n",
      "Step 19: val loss: 19.468564987182617\n",
      "Step 20: train loss: 20.239439010620117\n",
      "Step 20: val loss: 19.173049926757812\n",
      "Step 21: train loss: 19.95302963256836\n",
      "Step 21: val loss: 18.887815475463867\n",
      "Step 22: train loss: 19.67499542236328\n",
      "Step 22: val loss: 18.611927032470703\n",
      "Step 23: train loss: 19.404619216918945\n",
      "Step 23: val loss: 18.344547271728516\n",
      "Step 24: train loss: 19.141284942626953\n",
      "Step 24: val loss: 18.085132598876953\n",
      "Step 25: train loss: 18.884443283081055\n",
      "Step 25: val loss: 17.832910537719727\n",
      "Step 26: train loss: 18.633617401123047\n",
      "Step 26: val loss: 17.587526321411133\n",
      "Step 27: train loss: 18.388389587402344\n",
      "Step 27: val loss: 17.348329544067383\n",
      "Step 28: train loss: 18.148391723632812\n",
      "Step 28: val loss: 17.11506462097168\n",
      "Step 29: train loss: 17.913297653198242\n",
      "Step 29: val loss: 16.887205123901367\n",
      "Step 30: train loss: 17.682815551757812\n",
      "Step 30: val loss: 16.66454315185547\n",
      "Step 31: train loss: 17.456697463989258\n",
      "Step 31: val loss: 16.446664810180664\n",
      "Step 32: train loss: 17.234708786010742\n",
      "Step 32: val loss: 16.233396530151367\n",
      "Step 33: train loss: 17.016651153564453\n",
      "Step 33: val loss: 16.0244083404541\n",
      "Step 34: train loss: 16.802345275878906\n",
      "Step 34: val loss: 15.819554328918457\n",
      "Step 35: train loss: 16.59162712097168\n",
      "Step 35: val loss: 15.618574142456055\n",
      "Step 36: train loss: 16.384349822998047\n",
      "Step 36: val loss: 15.421338081359863\n",
      "Step 37: train loss: 16.180387496948242\n",
      "Step 37: val loss: 15.227643013000488\n",
      "Step 38: train loss: 15.979619026184082\n",
      "Step 38: val loss: 15.037367820739746\n",
      "Step 39: train loss: 15.781935691833496\n",
      "Step 39: val loss: 14.850356101989746\n",
      "Step 40: train loss: 15.587241172790527\n",
      "Step 40: val loss: 14.666500091552734\n",
      "Step 41: train loss: 15.3954439163208\n",
      "Step 41: val loss: 14.485671043395996\n",
      "Step 42: train loss: 15.206465721130371\n",
      "Step 42: val loss: 14.307777404785156\n",
      "Step 43: train loss: 15.020228385925293\n",
      "Step 43: val loss: 14.132709503173828\n",
      "Step 44: train loss: 14.836657524108887\n",
      "Step 44: val loss: 13.9603853225708\n",
      "Step 45: train loss: 14.655695915222168\n",
      "Step 45: val loss: 13.790721893310547\n",
      "Step 46: train loss: 14.477280616760254\n",
      "Step 46: val loss: 13.623639106750488\n",
      "Step 47: train loss: 14.301353454589844\n",
      "Step 47: val loss: 13.459059715270996\n",
      "Step 48: train loss: 14.127862930297852\n",
      "Step 48: val loss: 13.296926498413086\n",
      "Step 49: train loss: 13.956757545471191\n",
      "Step 49: val loss: 13.137168884277344\n",
      "Step 50: train loss: 13.787991523742676\n",
      "Step 50: val loss: 12.979732513427734\n",
      "Step 51: train loss: 13.621520042419434\n",
      "Step 51: val loss: 12.824555397033691\n",
      "Step 52: train loss: 13.457297325134277\n",
      "Step 52: val loss: 12.671588897705078\n",
      "Step 53: train loss: 13.2952880859375\n",
      "Step 53: val loss: 12.520781517028809\n",
      "Step 54: train loss: 13.135448455810547\n",
      "Step 54: val loss: 12.372084617614746\n",
      "Step 55: train loss: 12.977743148803711\n",
      "Step 55: val loss: 12.225455284118652\n",
      "Step 56: train loss: 12.822134971618652\n",
      "Step 56: val loss: 12.080849647521973\n",
      "Step 57: train loss: 12.668591499328613\n",
      "Step 57: val loss: 11.938227653503418\n",
      "Step 58: train loss: 12.517075538635254\n",
      "Step 58: val loss: 11.79754638671875\n",
      "Step 59: train loss: 12.367558479309082\n",
      "Step 59: val loss: 11.658771514892578\n",
      "Step 60: train loss: 12.220006942749023\n",
      "Step 60: val loss: 11.521866798400879\n",
      "Step 61: train loss: 12.074389457702637\n",
      "Step 61: val loss: 11.386794090270996\n",
      "Step 62: train loss: 11.93067741394043\n",
      "Step 62: val loss: 11.253523826599121\n",
      "Step 63: train loss: 11.788844108581543\n",
      "Step 63: val loss: 11.12202262878418\n",
      "Step 64: train loss: 11.648857116699219\n",
      "Step 64: val loss: 10.992258071899414\n",
      "Step 65: train loss: 11.510690689086914\n",
      "Step 65: val loss: 10.864201545715332\n",
      "Step 66: train loss: 11.374317169189453\n",
      "Step 66: val loss: 10.737824440002441\n",
      "Step 67: train loss: 11.239712715148926\n",
      "Step 67: val loss: 10.61309814453125\n",
      "Step 68: train loss: 11.106849670410156\n",
      "Step 68: val loss: 10.48999309539795\n",
      "Step 69: train loss: 10.975700378417969\n",
      "Step 69: val loss: 10.368486404418945\n",
      "Step 70: train loss: 10.846243858337402\n",
      "Step 70: val loss: 10.248547554016113\n",
      "Step 71: train loss: 10.718454360961914\n",
      "Step 71: val loss: 10.130156517028809\n",
      "Step 72: train loss: 10.592308044433594\n",
      "Step 72: val loss: 10.013286590576172\n",
      "Step 73: train loss: 10.467782020568848\n",
      "Step 73: val loss: 9.897912979125977\n",
      "Step 74: train loss: 10.34485149383545\n",
      "Step 74: val loss: 9.784013748168945\n",
      "Step 75: train loss: 10.223498344421387\n",
      "Step 75: val loss: 9.671566009521484\n",
      "Step 76: train loss: 10.103693008422852\n",
      "Step 76: val loss: 9.560547828674316\n",
      "Step 77: train loss: 9.985422134399414\n",
      "Step 77: val loss: 9.450937271118164\n",
      "Step 78: train loss: 9.868657112121582\n",
      "Step 78: val loss: 9.342713356018066\n",
      "Step 79: train loss: 9.753382682800293\n",
      "Step 79: val loss: 9.235857009887695\n",
      "Step 80: train loss: 9.639575004577637\n",
      "Step 80: val loss: 9.13034439086914\n",
      "Step 81: train loss: 9.527212142944336\n",
      "Step 81: val loss: 9.026159286499023\n",
      "Step 82: train loss: 9.416277885437012\n",
      "Step 82: val loss: 8.923279762268066\n",
      "Step 83: train loss: 9.306750297546387\n",
      "Step 83: val loss: 8.82168960571289\n",
      "Step 84: train loss: 9.198610305786133\n",
      "Step 84: val loss: 8.721368789672852\n",
      "Step 85: train loss: 9.091839790344238\n",
      "Step 85: val loss: 8.622299194335938\n",
      "Step 86: train loss: 8.986417770385742\n",
      "Step 86: val loss: 8.524462699890137\n",
      "Step 87: train loss: 8.882328033447266\n",
      "Step 87: val loss: 8.42784309387207\n",
      "Step 88: train loss: 8.77955150604248\n",
      "Step 88: val loss: 8.332420349121094\n",
      "Step 89: train loss: 8.678069114685059\n",
      "Step 89: val loss: 8.238181114196777\n",
      "Step 90: train loss: 8.577864646911621\n",
      "Step 90: val loss: 8.14510726928711\n",
      "Step 91: train loss: 8.478919982910156\n",
      "Step 91: val loss: 8.053181648254395\n",
      "Step 92: train loss: 8.381218910217285\n",
      "Step 92: val loss: 7.962391376495361\n",
      "Step 93: train loss: 8.284743309020996\n",
      "Step 93: val loss: 7.87271785736084\n",
      "Step 94: train loss: 8.189477920532227\n",
      "Step 94: val loss: 7.784145355224609\n",
      "Step 95: train loss: 8.095405578613281\n",
      "Step 95: val loss: 7.696661472320557\n",
      "Step 96: train loss: 8.002508163452148\n",
      "Step 96: val loss: 7.6102495193481445\n",
      "Step 97: train loss: 7.910773754119873\n",
      "Step 97: val loss: 7.524895191192627\n",
      "Step 98: train loss: 7.820184230804443\n",
      "Step 98: val loss: 7.440584182739258\n",
      "Step 99: train loss: 7.7307233810424805\n",
      "Step 99: val loss: 7.357302188873291\n",
      "Step 100: train loss: 7.642377853393555\n",
      "Step 100: val loss: 7.2750372886657715\n",
      "Step 101: train loss: 7.555134296417236\n",
      "Step 101: val loss: 7.1937713623046875\n",
      "Step 102: train loss: 7.468973159790039\n",
      "Step 102: val loss: 7.11349630355835\n",
      "Step 103: train loss: 7.383882522583008\n",
      "Step 103: val loss: 7.0341949462890625\n",
      "Step 104: train loss: 7.299849987030029\n",
      "Step 104: val loss: 6.955856800079346\n",
      "Step 105: train loss: 7.216858386993408\n",
      "Step 105: val loss: 6.8784661293029785\n",
      "Step 106: train loss: 7.134894371032715\n",
      "Step 106: val loss: 6.8020124435424805\n",
      "Step 107: train loss: 7.053945541381836\n",
      "Step 107: val loss: 6.726484298706055\n",
      "Step 108: train loss: 6.973997592926025\n",
      "Step 108: val loss: 6.651866436004639\n",
      "Step 109: train loss: 6.8950371742248535\n",
      "Step 109: val loss: 6.578149795532227\n",
      "Step 110: train loss: 6.817051887512207\n",
      "Step 110: val loss: 6.505322456359863\n",
      "Step 111: train loss: 6.740027904510498\n",
      "Step 111: val loss: 6.433371543884277\n",
      "Step 112: train loss: 6.6639533042907715\n",
      "Step 112: val loss: 6.3622846603393555\n",
      "Step 113: train loss: 6.588815212249756\n",
      "Step 113: val loss: 6.292051315307617\n",
      "Step 114: train loss: 6.514601230621338\n",
      "Step 114: val loss: 6.222660541534424\n",
      "Step 115: train loss: 6.441298007965088\n",
      "Step 115: val loss: 6.154101371765137\n",
      "Step 116: train loss: 6.368895053863525\n",
      "Step 116: val loss: 6.086363315582275\n",
      "Step 117: train loss: 6.297379016876221\n",
      "Step 117: val loss: 6.019435882568359\n",
      "Step 118: train loss: 6.226741313934326\n",
      "Step 118: val loss: 5.95330810546875\n",
      "Step 119: train loss: 6.1569671630859375\n",
      "Step 119: val loss: 5.887969970703125\n",
      "Step 120: train loss: 6.088046073913574\n",
      "Step 120: val loss: 5.823409557342529\n",
      "Step 121: train loss: 6.019967555999756\n",
      "Step 121: val loss: 5.759618759155273\n",
      "Step 122: train loss: 5.952719688415527\n",
      "Step 122: val loss: 5.696587562561035\n",
      "Step 123: train loss: 5.886293888092041\n",
      "Step 123: val loss: 5.63430643081665\n",
      "Step 124: train loss: 5.820677280426025\n",
      "Step 124: val loss: 5.572765350341797\n",
      "Step 125: train loss: 5.755859851837158\n",
      "Step 125: val loss: 5.511954307556152\n",
      "Step 126: train loss: 5.691831111907959\n",
      "Step 126: val loss: 5.451865196228027\n",
      "Step 127: train loss: 5.628581523895264\n",
      "Step 127: val loss: 5.392486572265625\n",
      "Step 128: train loss: 5.566098690032959\n",
      "Step 128: val loss: 5.3338117599487305\n",
      "Step 129: train loss: 5.5043745040893555\n",
      "Step 129: val loss: 5.275830268859863\n",
      "Step 130: train loss: 5.443398952484131\n",
      "Step 130: val loss: 5.218533992767334\n",
      "Step 131: train loss: 5.383163928985596\n",
      "Step 131: val loss: 5.161914825439453\n",
      "Step 132: train loss: 5.3236565589904785\n",
      "Step 132: val loss: 5.105963230133057\n",
      "Step 133: train loss: 5.264869213104248\n",
      "Step 133: val loss: 5.050671100616455\n",
      "Step 134: train loss: 5.206793308258057\n",
      "Step 134: val loss: 4.996029376983643\n",
      "Step 135: train loss: 5.149417877197266\n",
      "Step 135: val loss: 4.942031383514404\n",
      "Step 136: train loss: 5.092735767364502\n",
      "Step 136: val loss: 4.888668060302734\n",
      "Step 137: train loss: 5.036736488342285\n",
      "Step 137: val loss: 4.835931777954102\n",
      "Step 138: train loss: 4.981411933898926\n",
      "Step 138: val loss: 4.783813953399658\n",
      "Step 139: train loss: 4.926753044128418\n",
      "Step 139: val loss: 4.732306957244873\n",
      "Step 140: train loss: 4.872752666473389\n",
      "Step 140: val loss: 4.681405544281006\n",
      "Step 141: train loss: 4.819400787353516\n",
      "Step 141: val loss: 4.631099224090576\n",
      "Step 142: train loss: 4.766691207885742\n",
      "Step 142: val loss: 4.581380844116211\n",
      "Step 143: train loss: 4.71461296081543\n",
      "Step 143: val loss: 4.532244682312012\n",
      "Step 144: train loss: 4.6631598472595215\n",
      "Step 144: val loss: 4.483680725097656\n",
      "Step 145: train loss: 4.612322807312012\n",
      "Step 145: val loss: 4.435685634613037\n",
      "Step 146: train loss: 4.5620951652526855\n",
      "Step 146: val loss: 4.388248920440674\n",
      "Step 147: train loss: 4.512467861175537\n",
      "Step 147: val loss: 4.341366291046143\n",
      "Step 148: train loss: 4.463434219360352\n",
      "Step 148: val loss: 4.295029640197754\n",
      "Step 149: train loss: 4.414987087249756\n",
      "Step 149: val loss: 4.249232292175293\n",
      "Step 150: train loss: 4.367117404937744\n",
      "Step 150: val loss: 4.203967094421387\n",
      "Step 151: train loss: 4.319819450378418\n",
      "Step 151: val loss: 4.159228324890137\n",
      "Step 152: train loss: 4.273085117340088\n",
      "Step 152: val loss: 4.11500883102417\n",
      "Step 153: train loss: 4.226907730102539\n",
      "Step 153: val loss: 4.071302890777588\n",
      "Step 154: train loss: 4.181280136108398\n",
      "Step 154: val loss: 4.028102397918701\n",
      "Step 155: train loss: 4.136195182800293\n",
      "Step 155: val loss: 3.9854047298431396\n",
      "Step 156: train loss: 4.091646194458008\n",
      "Step 156: val loss: 3.94320011138916\n",
      "Step 157: train loss: 4.047626495361328\n",
      "Step 157: val loss: 3.901484727859497\n",
      "Step 158: train loss: 4.004129886627197\n",
      "Step 158: val loss: 3.8602514266967773\n",
      "Step 159: train loss: 3.961148738861084\n",
      "Step 159: val loss: 3.8194942474365234\n",
      "Step 160: train loss: 3.9186770915985107\n",
      "Step 160: val loss: 3.779208183288574\n",
      "Step 161: train loss: 3.8767082691192627\n",
      "Step 161: val loss: 3.7393875122070312\n",
      "Step 162: train loss: 3.835237741470337\n",
      "Step 162: val loss: 3.700026035308838\n",
      "Step 163: train loss: 3.7942566871643066\n",
      "Step 163: val loss: 3.661118268966675\n",
      "Step 164: train loss: 3.7537598609924316\n",
      "Step 164: val loss: 3.6226580142974854\n",
      "Step 165: train loss: 3.7137420177459717\n",
      "Step 165: val loss: 3.5846407413482666\n",
      "Step 166: train loss: 3.674196243286133\n",
      "Step 166: val loss: 3.5470614433288574\n",
      "Step 167: train loss: 3.635117530822754\n",
      "Step 167: val loss: 3.509913444519043\n",
      "Step 168: train loss: 3.596498966217041\n",
      "Step 168: val loss: 3.4731922149658203\n",
      "Step 169: train loss: 3.5583362579345703\n",
      "Step 169: val loss: 3.4368927478790283\n",
      "Step 170: train loss: 3.5206222534179688\n",
      "Step 170: val loss: 3.4010095596313477\n",
      "Step 171: train loss: 3.483351707458496\n",
      "Step 171: val loss: 3.3655381202697754\n",
      "Step 172: train loss: 3.4465203285217285\n",
      "Step 172: val loss: 3.3304734230041504\n",
      "Step 173: train loss: 3.410120964050293\n",
      "Step 173: val loss: 3.2958104610443115\n",
      "Step 174: train loss: 3.374150037765503\n",
      "Step 174: val loss: 3.2615444660186768\n",
      "Step 175: train loss: 3.3386011123657227\n",
      "Step 175: val loss: 3.2276697158813477\n",
      "Step 176: train loss: 3.303468704223633\n",
      "Step 176: val loss: 3.194183349609375\n",
      "Step 177: train loss: 3.2687485218048096\n",
      "Step 177: val loss: 3.1610796451568604\n",
      "Step 178: train loss: 3.234435796737671\n",
      "Step 178: val loss: 3.1283528804779053\n",
      "Step 179: train loss: 3.200523614883423\n",
      "Step 179: val loss: 3.096001386642456\n",
      "Step 180: train loss: 3.1670095920562744\n",
      "Step 180: val loss: 3.0640175342559814\n",
      "Step 181: train loss: 3.1338863372802734\n",
      "Step 181: val loss: 3.0323991775512695\n",
      "Step 182: train loss: 3.1011507511138916\n",
      "Step 182: val loss: 3.00114107131958\n",
      "Step 183: train loss: 3.0687975883483887\n",
      "Step 183: val loss: 2.9702389240264893\n",
      "Step 184: train loss: 3.0368223190307617\n",
      "Step 184: val loss: 2.9396889209747314\n",
      "Step 185: train loss: 3.0052199363708496\n",
      "Step 185: val loss: 2.9094860553741455\n",
      "Step 186: train loss: 2.9739866256713867\n",
      "Step 186: val loss: 2.879626750946045\n",
      "Step 187: train loss: 2.9431166648864746\n",
      "Step 187: val loss: 2.8501064777374268\n",
      "Step 188: train loss: 2.9126062393188477\n",
      "Step 188: val loss: 2.8209221363067627\n",
      "Step 189: train loss: 2.882451057434082\n",
      "Step 189: val loss: 2.7920684814453125\n",
      "Step 190: train loss: 2.852646827697754\n",
      "Step 190: val loss: 2.763542890548706\n",
      "Step 191: train loss: 2.8231887817382812\n",
      "Step 191: val loss: 2.7353403568267822\n",
      "Step 192: train loss: 2.7940731048583984\n",
      "Step 192: val loss: 2.7074575424194336\n",
      "Step 193: train loss: 2.76529598236084\n",
      "Step 193: val loss: 2.6798906326293945\n",
      "Step 194: train loss: 2.7368526458740234\n",
      "Step 194: val loss: 2.6526358127593994\n",
      "Step 195: train loss: 2.7087390422821045\n",
      "Step 195: val loss: 2.625689744949341\n",
      "Step 196: train loss: 2.680952787399292\n",
      "Step 196: val loss: 2.5990488529205322\n",
      "Step 197: train loss: 2.65348744392395\n",
      "Step 197: val loss: 2.572709321975708\n",
      "Step 198: train loss: 2.626340627670288\n",
      "Step 198: val loss: 2.5466668605804443\n",
      "Step 199: train loss: 2.5995073318481445\n",
      "Step 199: val loss: 2.520918369293213\n",
      "Step 200: train loss: 2.5729849338531494\n",
      "Step 200: val loss: 2.4954609870910645\n",
      "Step 201: train loss: 2.5467689037323\n",
      "Step 201: val loss: 2.4702908992767334\n",
      "Step 202: train loss: 2.5208561420440674\n",
      "Step 202: val loss: 2.445404529571533\n",
      "Step 203: train loss: 2.4952428340911865\n",
      "Step 203: val loss: 2.420799493789673\n",
      "Step 204: train loss: 2.4699254035949707\n",
      "Step 204: val loss: 2.3964710235595703\n",
      "Step 205: train loss: 2.444899797439575\n",
      "Step 205: val loss: 2.3724169731140137\n",
      "Step 206: train loss: 2.4201629161834717\n",
      "Step 206: val loss: 2.3486337661743164\n",
      "Step 207: train loss: 2.395711660385132\n",
      "Step 207: val loss: 2.3251187801361084\n",
      "Step 208: train loss: 2.371542453765869\n",
      "Step 208: val loss: 2.301867723464966\n",
      "Step 209: train loss: 2.3476510047912598\n",
      "Step 209: val loss: 2.2788782119750977\n",
      "Step 210: train loss: 2.3240346908569336\n",
      "Step 210: val loss: 2.2561469078063965\n",
      "Step 211: train loss: 2.300690174102783\n",
      "Step 211: val loss: 2.2336719036102295\n",
      "Step 212: train loss: 2.2776143550872803\n",
      "Step 212: val loss: 2.211448907852173\n",
      "Step 213: train loss: 2.2548036575317383\n",
      "Step 213: val loss: 2.1894752979278564\n",
      "Step 214: train loss: 2.232254981994629\n",
      "Step 214: val loss: 2.1677486896514893\n",
      "Step 215: train loss: 2.209965229034424\n",
      "Step 215: val loss: 2.146265745162964\n",
      "Step 216: train loss: 2.187931776046753\n",
      "Step 216: val loss: 2.125023126602173\n",
      "Step 217: train loss: 2.1661503314971924\n",
      "Step 217: val loss: 2.1040189266204834\n",
      "Step 218: train loss: 2.1446192264556885\n",
      "Step 218: val loss: 2.083249807357788\n",
      "Step 219: train loss: 2.1233346462249756\n",
      "Step 219: val loss: 2.062713384628296\n",
      "Step 220: train loss: 2.1022939682006836\n",
      "Step 220: val loss: 2.0424067974090576\n",
      "Step 221: train loss: 2.0814945697784424\n",
      "Step 221: val loss: 2.0223281383514404\n",
      "Step 222: train loss: 2.0609328746795654\n",
      "Step 222: val loss: 2.0024735927581787\n",
      "Step 223: train loss: 2.04060697555542\n",
      "Step 223: val loss: 1.9828413724899292\n",
      "Step 224: train loss: 2.0205135345458984\n",
      "Step 224: val loss: 1.9634279012680054\n",
      "Step 225: train loss: 2.0006489753723145\n",
      "Step 225: val loss: 1.9442315101623535\n",
      "Step 226: train loss: 1.9810116291046143\n",
      "Step 226: val loss: 1.925249457359314\n",
      "Step 227: train loss: 1.9615988731384277\n",
      "Step 227: val loss: 1.906480073928833\n",
      "Step 228: train loss: 1.9424079656600952\n",
      "Step 228: val loss: 1.8879196643829346\n",
      "Step 229: train loss: 1.9234355688095093\n",
      "Step 229: val loss: 1.8695663213729858\n",
      "Step 230: train loss: 1.9046798944473267\n",
      "Step 230: val loss: 1.8514182567596436\n",
      "Step 231: train loss: 1.8861383199691772\n",
      "Step 231: val loss: 1.8334717750549316\n",
      "Step 232: train loss: 1.8678075075149536\n",
      "Step 232: val loss: 1.8157262802124023\n",
      "Step 233: train loss: 1.8496862649917603\n",
      "Step 233: val loss: 1.7981774806976318\n",
      "Step 234: train loss: 1.831770658493042\n",
      "Step 234: val loss: 1.780824899673462\n",
      "Step 235: train loss: 1.8140596151351929\n",
      "Step 235: val loss: 1.7636656761169434\n",
      "Step 236: train loss: 1.7965502738952637\n",
      "Step 236: val loss: 1.746696949005127\n",
      "Step 237: train loss: 1.7792396545410156\n",
      "Step 237: val loss: 1.729917049407959\n",
      "Step 238: train loss: 1.762125849723816\n",
      "Step 238: val loss: 1.7133240699768066\n",
      "Step 239: train loss: 1.7452069520950317\n",
      "Step 239: val loss: 1.6969153881072998\n",
      "Step 240: train loss: 1.7284796237945557\n",
      "Step 240: val loss: 1.680688738822937\n",
      "Step 241: train loss: 1.7119427919387817\n",
      "Step 241: val loss: 1.6646429300308228\n",
      "Step 242: train loss: 1.6955933570861816\n",
      "Step 242: val loss: 1.6487752199172974\n",
      "Step 243: train loss: 1.679429531097412\n",
      "Step 243: val loss: 1.633083701133728\n",
      "Step 244: train loss: 1.6634490489959717\n",
      "Step 244: val loss: 1.617566704750061\n",
      "Step 245: train loss: 1.647650122642517\n",
      "Step 245: val loss: 1.6022214889526367\n",
      "Step 246: train loss: 1.6320300102233887\n",
      "Step 246: val loss: 1.5870469808578491\n",
      "Step 247: train loss: 1.6165869235992432\n",
      "Step 247: val loss: 1.5720405578613281\n",
      "Step 248: train loss: 1.6013188362121582\n",
      "Step 248: val loss: 1.5572004318237305\n",
      "Step 249: train loss: 1.5862237215042114\n",
      "Step 249: val loss: 1.542525291442871\n",
      "Step 250: train loss: 1.5712990760803223\n",
      "Step 250: val loss: 1.5280125141143799\n",
      "Step 251: train loss: 1.5565438270568848\n",
      "Step 251: val loss: 1.5136607885360718\n",
      "Step 252: train loss: 1.5419554710388184\n",
      "Step 252: val loss: 1.4994679689407349\n",
      "Step 253: train loss: 1.5275317430496216\n",
      "Step 253: val loss: 1.4854321479797363\n",
      "Step 254: train loss: 1.5132712125778198\n",
      "Step 254: val loss: 1.4715516567230225\n",
      "Step 255: train loss: 1.4991718530654907\n",
      "Step 255: val loss: 1.4578243494033813\n",
      "Step 256: train loss: 1.4852315187454224\n",
      "Step 256: val loss: 1.4442496299743652\n",
      "Step 257: train loss: 1.471448540687561\n",
      "Step 257: val loss: 1.430824875831604\n",
      "Step 258: train loss: 1.4578213691711426\n",
      "Step 258: val loss: 1.417548656463623\n",
      "Step 259: train loss: 1.4443479776382446\n",
      "Step 259: val loss: 1.4044194221496582\n",
      "Step 260: train loss: 1.431026577949524\n",
      "Step 260: val loss: 1.391434669494629\n",
      "Step 261: train loss: 1.4178550243377686\n",
      "Step 261: val loss: 1.3785940408706665\n",
      "Step 262: train loss: 1.4048324823379517\n",
      "Step 262: val loss: 1.3658946752548218\n",
      "Step 263: train loss: 1.3919559717178345\n",
      "Step 263: val loss: 1.3533356189727783\n",
      "Step 264: train loss: 1.3792250156402588\n",
      "Step 264: val loss: 1.3409152030944824\n",
      "Step 265: train loss: 1.366637110710144\n",
      "Step 265: val loss: 1.3286322355270386\n",
      "Step 266: train loss: 1.3541913032531738\n",
      "Step 266: val loss: 1.3164846897125244\n",
      "Step 267: train loss: 1.3418850898742676\n",
      "Step 267: val loss: 1.3044707775115967\n",
      "Step 268: train loss: 1.329717755317688\n",
      "Step 268: val loss: 1.2925901412963867\n",
      "Step 269: train loss: 1.3176870346069336\n",
      "Step 269: val loss: 1.2808398008346558\n",
      "Step 270: train loss: 1.3057914972305298\n",
      "Step 270: val loss: 1.2692196369171143\n",
      "Step 271: train loss: 1.2940301895141602\n",
      "Step 271: val loss: 1.2577272653579712\n",
      "Step 272: train loss: 1.2824004888534546\n",
      "Step 272: val loss: 1.2463619709014893\n",
      "Step 273: train loss: 1.270902156829834\n",
      "Step 273: val loss: 1.235121488571167\n",
      "Step 274: train loss: 1.2595322132110596\n",
      "Step 274: val loss: 1.2240046262741089\n",
      "Step 275: train loss: 1.248289942741394\n",
      "Step 275: val loss: 1.2130104303359985\n",
      "Step 276: train loss: 1.2371740341186523\n",
      "Step 276: val loss: 1.2021373510360718\n",
      "Step 277: train loss: 1.2261828184127808\n",
      "Step 277: val loss: 1.1913840770721436\n",
      "Step 278: train loss: 1.2153149843215942\n",
      "Step 278: val loss: 1.180748701095581\n",
      "Step 279: train loss: 1.2045689821243286\n",
      "Step 279: val loss: 1.170230746269226\n",
      "Step 280: train loss: 1.1939433813095093\n",
      "Step 280: val loss: 1.1598286628723145\n",
      "Step 281: train loss: 1.1834369897842407\n",
      "Step 281: val loss: 1.149540662765503\n",
      "Step 282: train loss: 1.1730481386184692\n",
      "Step 282: val loss: 1.139365792274475\n",
      "Step 283: train loss: 1.1627758741378784\n",
      "Step 283: val loss: 1.129302978515625\n",
      "Step 284: train loss: 1.1526185274124146\n",
      "Step 284: val loss: 1.119350790977478\n",
      "Step 285: train loss: 1.1425751447677612\n",
      "Step 285: val loss: 1.1095083951950073\n",
      "Step 286: train loss: 1.1326439380645752\n",
      "Step 286: val loss: 1.0997737646102905\n",
      "Step 287: train loss: 1.1228240728378296\n",
      "Step 287: val loss: 1.0901458263397217\n",
      "Step 288: train loss: 1.1131136417388916\n",
      "Step 288: val loss: 1.080623984336853\n",
      "Step 289: train loss: 1.103512167930603\n",
      "Step 289: val loss: 1.0712064504623413\n",
      "Step 290: train loss: 1.094017505645752\n",
      "Step 290: val loss: 1.0618926286697388\n",
      "Step 291: train loss: 1.0846295356750488\n",
      "Step 291: val loss: 1.0526807308197021\n",
      "Step 292: train loss: 1.0753467082977295\n",
      "Step 292: val loss: 1.0435703992843628\n",
      "Step 293: train loss: 1.0661671161651611\n",
      "Step 293: val loss: 1.0345596075057983\n",
      "Step 294: train loss: 1.0570898056030273\n",
      "Step 294: val loss: 1.025647759437561\n",
      "Step 295: train loss: 1.04811429977417\n",
      "Step 295: val loss: 1.0168335437774658\n",
      "Step 296: train loss: 1.0392389297485352\n",
      "Step 296: val loss: 1.0081161260604858\n",
      "Step 297: train loss: 1.0304621458053589\n",
      "Step 297: val loss: 0.9994940161705017\n",
      "Step 298: train loss: 1.0217833518981934\n",
      "Step 298: val loss: 0.9909668564796448\n",
      "Step 299: train loss: 1.0132015943527222\n",
      "Step 299: val loss: 0.9825330376625061\n",
      "Step 300: train loss: 1.0047155618667603\n",
      "Step 300: val loss: 0.9741919040679932\n",
      "Step 301: train loss: 0.9963242411613464\n",
      "Step 301: val loss: 0.9659420251846313\n",
      "Step 302: train loss: 0.9880263805389404\n",
      "Step 302: val loss: 0.9577826857566833\n",
      "Step 303: train loss: 0.9798210859298706\n",
      "Step 303: val loss: 0.9497122764587402\n",
      "Step 304: train loss: 0.9717069864273071\n",
      "Step 304: val loss: 0.9417304992675781\n",
      "Step 305: train loss: 0.9636831879615784\n",
      "Step 305: val loss: 0.933836042881012\n",
      "Step 306: train loss: 0.9557487964630127\n",
      "Step 306: val loss: 0.926028311252594\n",
      "Step 307: train loss: 0.9479028582572937\n",
      "Step 307: val loss: 0.9183056354522705\n",
      "Step 308: train loss: 0.9401441812515259\n",
      "Step 308: val loss: 0.9106680154800415\n",
      "Step 309: train loss: 0.932471752166748\n",
      "Step 309: val loss: 0.903113603591919\n",
      "Step 310: train loss: 0.9248849749565125\n",
      "Step 310: val loss: 0.8956420421600342\n",
      "Step 311: train loss: 0.9173824191093445\n",
      "Step 311: val loss: 0.8882522583007812\n",
      "Step 312: train loss: 0.9099633097648621\n",
      "Step 312: val loss: 0.8809434771537781\n",
      "Step 313: train loss: 0.9026268720626831\n",
      "Step 313: val loss: 0.8737143874168396\n",
      "Step 314: train loss: 0.895371675491333\n",
      "Step 314: val loss: 0.8665646314620972\n",
      "Step 315: train loss: 0.8881974816322327\n",
      "Step 315: val loss: 0.8594926595687866\n",
      "Step 316: train loss: 0.8811027407646179\n",
      "Step 316: val loss: 0.8524982929229736\n",
      "Step 317: train loss: 0.8740869164466858\n",
      "Step 317: val loss: 0.8455806970596313\n",
      "Step 318: train loss: 0.8671490550041199\n",
      "Step 318: val loss: 0.8387383222579956\n",
      "Step 319: train loss: 0.8602880835533142\n",
      "Step 319: val loss: 0.8319708108901978\n",
      "Step 320: train loss: 0.8535033464431763\n",
      "Step 320: val loss: 0.8252773284912109\n",
      "Step 321: train loss: 0.8467939496040344\n",
      "Step 321: val loss: 0.8186567425727844\n",
      "Step 322: train loss: 0.8401589393615723\n",
      "Step 322: val loss: 0.8121086955070496\n",
      "Step 323: train loss: 0.8335975408554077\n",
      "Step 323: val loss: 0.8056321740150452\n",
      "Step 324: train loss: 0.827109158039093\n",
      "Step 324: val loss: 0.7992267608642578\n",
      "Step 325: train loss: 0.8206928372383118\n",
      "Step 325: val loss: 0.792890727519989\n",
      "Step 326: train loss: 0.8143472075462341\n",
      "Step 326: val loss: 0.786624014377594\n",
      "Step 327: train loss: 0.8080721497535706\n",
      "Step 327: val loss: 0.7804262042045593\n",
      "Step 328: train loss: 0.8018667697906494\n",
      "Step 328: val loss: 0.7742958068847656\n",
      "Step 329: train loss: 0.7957300543785095\n",
      "Step 329: val loss: 0.768231987953186\n",
      "Step 330: train loss: 0.7896612286567688\n",
      "Step 330: val loss: 0.7622348070144653\n",
      "Step 331: train loss: 0.783659815788269\n",
      "Step 331: val loss: 0.7563029527664185\n",
      "Step 332: train loss: 0.7777248620986938\n",
      "Step 332: val loss: 0.7504358887672424\n",
      "Step 333: train loss: 0.7718555927276611\n",
      "Step 333: val loss: 0.7446326613426208\n",
      "Step 334: train loss: 0.7660511136054993\n",
      "Step 334: val loss: 0.7388927936553955\n",
      "Step 335: train loss: 0.76031094789505\n",
      "Step 335: val loss: 0.7332156300544739\n",
      "Step 336: train loss: 0.7546344995498657\n",
      "Step 336: val loss: 0.7276002764701843\n",
      "Step 337: train loss: 0.7490206956863403\n",
      "Step 337: val loss: 0.7220461964607239\n",
      "Step 338: train loss: 0.7434690594673157\n",
      "Step 338: val loss: 0.716552734375\n",
      "Step 339: train loss: 0.7379788160324097\n",
      "Step 339: val loss: 0.7111194133758545\n",
      "Step 340: train loss: 0.7325496077537537\n",
      "Step 340: val loss: 0.7057452201843262\n",
      "Step 341: train loss: 0.7271801233291626\n",
      "Step 341: val loss: 0.7004296183586121\n",
      "Step 342: train loss: 0.7218700647354126\n",
      "Step 342: val loss: 0.6951717138290405\n",
      "Step 343: train loss: 0.7166185975074768\n",
      "Step 343: val loss: 0.6899710893630981\n",
      "Step 344: train loss: 0.7114250659942627\n",
      "Step 344: val loss: 0.684827446937561\n",
      "Step 345: train loss: 0.7062894701957703\n",
      "Step 345: val loss: 0.6797398328781128\n",
      "Step 346: train loss: 0.7012102007865906\n",
      "Step 346: val loss: 0.6747077703475952\n",
      "Step 347: train loss: 0.6961871981620789\n",
      "Step 347: val loss: 0.669730544090271\n",
      "Step 348: train loss: 0.6912196278572083\n",
      "Step 348: val loss: 0.6648076772689819\n",
      "Step 349: train loss: 0.6863071322441101\n",
      "Step 349: val loss: 0.6599380970001221\n",
      "Step 350: train loss: 0.6814486980438232\n",
      "Step 350: val loss: 0.6551216840744019\n",
      "Step 351: train loss: 0.6766439080238342\n",
      "Step 351: val loss: 0.6503579020500183\n",
      "Step 352: train loss: 0.6718922257423401\n",
      "Step 352: val loss: 0.6456460356712341\n",
      "Step 353: train loss: 0.6671929955482483\n",
      "Step 353: val loss: 0.6409850120544434\n",
      "Step 354: train loss: 0.6625455617904663\n",
      "Step 354: val loss: 0.636375367641449\n",
      "Step 355: train loss: 0.6579493880271912\n",
      "Step 355: val loss: 0.6318157315254211\n",
      "Step 356: train loss: 0.6534039974212646\n",
      "Step 356: val loss: 0.6273056268692017\n",
      "Step 357: train loss: 0.6489086151123047\n",
      "Step 357: val loss: 0.6228448152542114\n",
      "Step 358: train loss: 0.6444627642631531\n",
      "Step 358: val loss: 0.6184322834014893\n",
      "Step 359: train loss: 0.6400660872459412\n",
      "Step 359: val loss: 0.6140679717063904\n",
      "Step 360: train loss: 0.6357178688049316\n",
      "Step 360: val loss: 0.6097512245178223\n",
      "Step 361: train loss: 0.6314175128936768\n",
      "Step 361: val loss: 0.6054813861846924\n",
      "Step 362: train loss: 0.6271646022796631\n",
      "Step 362: val loss: 0.6012579202651978\n",
      "Step 363: train loss: 0.6229586005210876\n",
      "Step 363: val loss: 0.597080647945404\n",
      "Step 364: train loss: 0.6187988519668579\n",
      "Step 364: val loss: 0.592948853969574\n",
      "Step 365: train loss: 0.61468505859375\n",
      "Step 365: val loss: 0.5888620615005493\n",
      "Step 366: train loss: 0.6106166839599609\n",
      "Step 366: val loss: 0.5848196148872375\n",
      "Step 367: train loss: 0.6065928936004639\n",
      "Step 367: val loss: 0.5808209180831909\n",
      "Step 368: train loss: 0.6026134490966797\n",
      "Step 368: val loss: 0.576866090297699\n",
      "Step 369: train loss: 0.5986777544021606\n",
      "Step 369: val loss: 0.5729541778564453\n",
      "Step 370: train loss: 0.5947855710983276\n",
      "Step 370: val loss: 0.5690850615501404\n",
      "Step 371: train loss: 0.5909362435340881\n",
      "Step 371: val loss: 0.5652580261230469\n",
      "Step 372: train loss: 0.5871293544769287\n",
      "Step 372: val loss: 0.5614722967147827\n",
      "Step 373: train loss: 0.5833643078804016\n",
      "Step 373: val loss: 0.5577279925346375\n",
      "Step 374: train loss: 0.5796406269073486\n",
      "Step 374: val loss: 0.554024338722229\n",
      "Step 375: train loss: 0.5759578943252563\n",
      "Step 375: val loss: 0.5503612756729126\n",
      "Step 376: train loss: 0.5723158717155457\n",
      "Step 376: val loss: 0.5467377305030823\n",
      "Step 377: train loss: 0.5687136650085449\n",
      "Step 377: val loss: 0.5431534647941589\n",
      "Step 378: train loss: 0.5651512742042542\n",
      "Step 378: val loss: 0.5396084785461426\n",
      "Step 379: train loss: 0.561627984046936\n",
      "Step 379: val loss: 0.5361016988754272\n",
      "Step 380: train loss: 0.5581433176994324\n",
      "Step 380: val loss: 0.5326331257820129\n",
      "Step 381: train loss: 0.5546970367431641\n",
      "Step 381: val loss: 0.5292023420333862\n",
      "Step 382: train loss: 0.5512886643409729\n",
      "Step 382: val loss: 0.5258089303970337\n",
      "Step 383: train loss: 0.5479176044464111\n",
      "Step 383: val loss: 0.5224524140357971\n",
      "Step 384: train loss: 0.5445838570594788\n",
      "Step 384: val loss: 0.5191325545310974\n",
      "Step 385: train loss: 0.5412866473197937\n",
      "Step 385: val loss: 0.5158485174179077\n",
      "Step 386: train loss: 0.5380257964134216\n",
      "Step 386: val loss: 0.5126002430915833\n",
      "Step 387: train loss: 0.5348005890846252\n",
      "Step 387: val loss: 0.509387195110321\n",
      "Step 388: train loss: 0.5316107869148254\n",
      "Step 388: val loss: 0.506209135055542\n",
      "Step 389: train loss: 0.5284559726715088\n",
      "Step 389: val loss: 0.5030657052993774\n",
      "Step 390: train loss: 0.5253360271453857\n",
      "Step 390: val loss: 0.4999563992023468\n",
      "Step 391: train loss: 0.5222501754760742\n",
      "Step 391: val loss: 0.49688076972961426\n",
      "Step 392: train loss: 0.5191982388496399\n",
      "Step 392: val loss: 0.4938388466835022\n",
      "Step 393: train loss: 0.5161798596382141\n",
      "Step 393: val loss: 0.4908297061920166\n",
      "Step 394: train loss: 0.5131945610046387\n",
      "Step 394: val loss: 0.4878537356853485\n",
      "Step 395: train loss: 0.5102421045303345\n",
      "Step 395: val loss: 0.48490965366363525\n",
      "Step 396: train loss: 0.5073221325874329\n",
      "Step 396: val loss: 0.48199784755706787\n",
      "Step 397: train loss: 0.5044339895248413\n",
      "Step 397: val loss: 0.47911736369132996\n",
      "Step 398: train loss: 0.5015776753425598\n",
      "Step 398: val loss: 0.4762685298919678\n",
      "Step 399: train loss: 0.49875277280807495\n",
      "Step 399: val loss: 0.47345027327537537\n",
      "Step 400: train loss: 0.49595877528190613\n",
      "Step 400: val loss: 0.4706629812717438\n",
      "Step 401: train loss: 0.49319538474082947\n",
      "Step 401: val loss: 0.4679056406021118\n",
      "Step 402: train loss: 0.49046239256858826\n",
      "Step 402: val loss: 0.46517863869667053\n",
      "Step 403: train loss: 0.48775941133499146\n",
      "Step 403: val loss: 0.46248096227645874\n",
      "Step 404: train loss: 0.48508599400520325\n",
      "Step 404: val loss: 0.45981281995773315\n",
      "Step 405: train loss: 0.48244181275367737\n",
      "Step 405: val loss: 0.4571733772754669\n",
      "Step 406: train loss: 0.4798266291618347\n",
      "Step 406: val loss: 0.45456284284591675\n",
      "Step 407: train loss: 0.47724026441574097\n",
      "Step 407: val loss: 0.4519808292388916\n",
      "Step 408: train loss: 0.4746822714805603\n",
      "Step 408: val loss: 0.4494266211986542\n",
      "Step 409: train loss: 0.4721521735191345\n",
      "Step 409: val loss: 0.4469001889228821\n",
      "Step 410: train loss: 0.46964988112449646\n",
      "Step 410: val loss: 0.44440117478370667\n",
      "Step 411: train loss: 0.4671749770641327\n",
      "Step 411: val loss: 0.44192928075790405\n",
      "Step 412: train loss: 0.4647272527217865\n",
      "Step 412: val loss: 0.43948447704315186\n",
      "Step 413: train loss: 0.4623062014579773\n",
      "Step 413: val loss: 0.43706607818603516\n",
      "Step 414: train loss: 0.45991194248199463\n",
      "Step 414: val loss: 0.4346742331981659\n",
      "Step 415: train loss: 0.4575437903404236\n",
      "Step 415: val loss: 0.43230801820755005\n",
      "Step 416: train loss: 0.4552016854286194\n",
      "Step 416: val loss: 0.4299677312374115\n",
      "Step 417: train loss: 0.4528851807117462\n",
      "Step 417: val loss: 0.42765262722969055\n",
      "Step 418: train loss: 0.45059406757354736\n",
      "Step 418: val loss: 0.42536309361457825\n",
      "Step 419: train loss: 0.4483279287815094\n",
      "Step 419: val loss: 0.42309805750846863\n",
      "Step 420: train loss: 0.4460867643356323\n",
      "Step 420: val loss: 0.4208577871322632\n",
      "Step 421: train loss: 0.44387009739875793\n",
      "Step 421: val loss: 0.41864174604415894\n",
      "Step 422: train loss: 0.44167760014533997\n",
      "Step 422: val loss: 0.4164499044418335\n",
      "Step 423: train loss: 0.4395093023777008\n",
      "Step 423: val loss: 0.4142819344997406\n",
      "Step 424: train loss: 0.4373646676540375\n",
      "Step 424: val loss: 0.4121372699737549\n",
      "Step 425: train loss: 0.4352433681488037\n",
      "Step 425: val loss: 0.4100160598754883\n",
      "Step 426: train loss: 0.43314528465270996\n",
      "Step 426: val loss: 0.4079175889492035\n",
      "Step 427: train loss: 0.4310701787471771\n",
      "Step 427: val loss: 0.40584197640419006\n",
      "Step 428: train loss: 0.4290177822113037\n",
      "Step 428: val loss: 0.40378910303115845\n",
      "Step 429: train loss: 0.42698800563812256\n",
      "Step 429: val loss: 0.40175846219062805\n",
      "Step 430: train loss: 0.4249803125858307\n",
      "Step 430: val loss: 0.39974987506866455\n",
      "Step 431: train loss: 0.4229944348335266\n",
      "Step 431: val loss: 0.39776310324668884\n",
      "Step 432: train loss: 0.4210304617881775\n",
      "Step 432: val loss: 0.3957979679107666\n",
      "Step 433: train loss: 0.4190879166126251\n",
      "Step 433: val loss: 0.3938542306423187\n",
      "Step 434: train loss: 0.41716668009757996\n",
      "Step 434: val loss: 0.3919313848018646\n",
      "Step 435: train loss: 0.4152664542198181\n",
      "Step 435: val loss: 0.3900296092033386\n",
      "Step 436: train loss: 0.4133870005607605\n",
      "Step 436: val loss: 0.3881484568119049\n",
      "Step 437: train loss: 0.4115281403064728\n",
      "Step 437: val loss: 0.38628777861595154\n",
      "Step 438: train loss: 0.4096895456314087\n",
      "Step 438: val loss: 0.38444721698760986\n",
      "Step 439: train loss: 0.4078710973262787\n",
      "Step 439: val loss: 0.3826265037059784\n",
      "Step 440: train loss: 0.4060724973678589\n",
      "Step 440: val loss: 0.38082581758499146\n",
      "Step 441: train loss: 0.4042935073375702\n",
      "Step 441: val loss: 0.37904471158981323\n",
      "Step 442: train loss: 0.4025341272354126\n",
      "Step 442: val loss: 0.3772829473018646\n",
      "Step 443: train loss: 0.4007938504219055\n",
      "Step 443: val loss: 0.37554019689559937\n",
      "Step 444: train loss: 0.3990727663040161\n",
      "Step 444: val loss: 0.37381651997566223\n",
      "Step 445: train loss: 0.3973706066608429\n",
      "Step 445: val loss: 0.37211132049560547\n",
      "Step 446: train loss: 0.39568665623664856\n",
      "Step 446: val loss: 0.37042495608329773\n",
      "Step 447: train loss: 0.39402133226394653\n",
      "Step 447: val loss: 0.36875665187835693\n",
      "Step 448: train loss: 0.392374187707901\n",
      "Step 448: val loss: 0.3671067953109741\n",
      "Step 449: train loss: 0.39074501395225525\n",
      "Step 449: val loss: 0.3654746115207672\n",
      "Step 450: train loss: 0.3891337513923645\n",
      "Step 450: val loss: 0.3638601303100586\n",
      "Step 451: train loss: 0.38753989338874817\n",
      "Step 451: val loss: 0.36226317286491394\n",
      "Step 452: train loss: 0.3859635591506958\n",
      "Step 452: val loss: 0.36068370938301086\n",
      "Step 453: train loss: 0.38440442085266113\n",
      "Step 453: val loss: 0.3591213524341583\n",
      "Step 454: train loss: 0.38286226987838745\n",
      "Step 454: val loss: 0.35757580399513245\n",
      "Step 455: train loss: 0.38133689761161804\n",
      "Step 455: val loss: 0.35604721307754517\n",
      "Step 456: train loss: 0.37982842326164246\n",
      "Step 456: val loss: 0.3545352816581726\n",
      "Step 457: train loss: 0.3783363699913025\n",
      "Step 457: val loss: 0.3530396521091461\n",
      "Step 458: train loss: 0.37686052918434143\n",
      "Step 458: val loss: 0.3515603244304657\n",
      "Step 459: train loss: 0.3754008412361145\n",
      "Step 459: val loss: 0.35009709000587463\n",
      "Step 460: train loss: 0.37395718693733215\n",
      "Step 460: val loss: 0.3486498296260834\n",
      "Step 461: train loss: 0.37252917885780334\n",
      "Step 461: val loss: 0.3472180664539337\n",
      "Step 462: train loss: 0.3711167573928833\n",
      "Step 462: val loss: 0.34580186009407043\n",
      "Step 463: train loss: 0.36971965432167053\n",
      "Step 463: val loss: 0.3444010019302368\n",
      "Step 464: train loss: 0.3683378994464874\n",
      "Step 464: val loss: 0.3430156409740448\n",
      "Step 465: train loss: 0.3669711947441101\n",
      "Step 465: val loss: 0.34164509177207947\n",
      "Step 466: train loss: 0.3656195104122162\n",
      "Step 466: val loss: 0.34028956294059753\n",
      "Step 467: train loss: 0.364282488822937\n",
      "Step 467: val loss: 0.3389487564563751\n",
      "Step 468: train loss: 0.3629600405693054\n",
      "Step 468: val loss: 0.3376225531101227\n",
      "Step 469: train loss: 0.361652135848999\n",
      "Step 469: val loss: 0.33631056547164917\n",
      "Step 470: train loss: 0.36035847663879395\n",
      "Step 470: val loss: 0.3350130021572113\n",
      "Step 471: train loss: 0.35907885432243347\n",
      "Step 471: val loss: 0.33372950553894043\n",
      "Step 472: train loss: 0.3578132688999176\n",
      "Step 472: val loss: 0.33245980739593506\n",
      "Step 473: train loss: 0.3565613031387329\n",
      "Step 473: val loss: 0.33120399713516235\n",
      "Step 474: train loss: 0.3553232252597809\n",
      "Step 474: val loss: 0.32996177673339844\n",
      "Step 475: train loss: 0.3540985882282257\n",
      "Step 475: val loss: 0.3287333548069\n",
      "Step 476: train loss: 0.35288727283477783\n",
      "Step 476: val loss: 0.32751786708831787\n",
      "Step 477: train loss: 0.3516892194747925\n",
      "Step 477: val loss: 0.3263160288333893\n",
      "Step 478: train loss: 0.3505043685436249\n",
      "Step 478: val loss: 0.3251270353794098\n",
      "Step 479: train loss: 0.3493322730064392\n",
      "Step 479: val loss: 0.3239511251449585\n",
      "Step 480: train loss: 0.34817323088645935\n",
      "Step 480: val loss: 0.32278773188591003\n",
      "Step 481: train loss: 0.3470264673233032\n",
      "Step 481: val loss: 0.3216370642185211\n",
      "Step 482: train loss: 0.3458923399448395\n",
      "Step 482: val loss: 0.3204990327358246\n",
      "Step 483: train loss: 0.344770610332489\n",
      "Step 483: val loss: 0.319373220205307\n",
      "Step 484: train loss: 0.3436610698699951\n",
      "Step 484: val loss: 0.3182598948478699\n",
      "Step 485: train loss: 0.34256377816200256\n",
      "Step 485: val loss: 0.3171583414077759\n",
      "Step 486: train loss: 0.34147825837135315\n",
      "Step 486: val loss: 0.31606873869895935\n",
      "Step 487: train loss: 0.3404046297073364\n",
      "Step 487: val loss: 0.3149912655353546\n",
      "Step 488: train loss: 0.33934271335601807\n",
      "Step 488: val loss: 0.31392550468444824\n",
      "Step 489: train loss: 0.3382924795150757\n",
      "Step 489: val loss: 0.3128710091114044\n",
      "Step 490: train loss: 0.337253600358963\n",
      "Step 490: val loss: 0.31182825565338135\n",
      "Step 491: train loss: 0.3362259566783905\n",
      "Step 491: val loss: 0.31079670786857605\n",
      "Step 492: train loss: 0.33520954847335815\n",
      "Step 492: val loss: 0.30977633595466614\n",
      "Step 493: train loss: 0.3342042863368988\n",
      "Step 493: val loss: 0.3087671995162964\n",
      "Step 494: train loss: 0.3332099914550781\n",
      "Step 494: val loss: 0.307768851518631\n",
      "Step 495: train loss: 0.3322264850139618\n",
      "Step 495: val loss: 0.3067815899848938\n",
      "Step 496: train loss: 0.33125370740890503\n",
      "Step 496: val loss: 0.3058047592639923\n",
      "Step 497: train loss: 0.33029159903526306\n",
      "Step 497: val loss: 0.30483904480934143\n",
      "Step 498: train loss: 0.3293399512767792\n",
      "Step 498: val loss: 0.30388346314430237\n",
      "Step 499: train loss: 0.3283987045288086\n",
      "Step 499: val loss: 0.302938312292099\n",
      "Step 500: train loss: 0.327467679977417\n",
      "Step 500: val loss: 0.3020033538341522\n",
      "Step 501: train loss: 0.3265467584133148\n",
      "Step 501: val loss: 0.301078736782074\n",
      "Step 502: train loss: 0.32563579082489014\n",
      "Step 502: val loss: 0.30016404390335083\n",
      "Step 503: train loss: 0.3247348666191101\n",
      "Step 503: val loss: 0.29925933480262756\n",
      "Step 504: train loss: 0.3238438069820404\n",
      "Step 504: val loss: 0.2983645498752594\n",
      "Step 505: train loss: 0.32296231389045715\n",
      "Step 505: val loss: 0.297479510307312\n",
      "Step 506: train loss: 0.32209059596061707\n",
      "Step 506: val loss: 0.2966039776802063\n",
      "Step 507: train loss: 0.3212282061576843\n",
      "Step 507: val loss: 0.29573798179626465\n",
      "Step 508: train loss: 0.3203752934932709\n",
      "Step 508: val loss: 0.29488158226013184\n",
      "Step 509: train loss: 0.31953173875808716\n",
      "Step 509: val loss: 0.29403427243232727\n",
      "Step 510: train loss: 0.3186973035335541\n",
      "Step 510: val loss: 0.29319632053375244\n",
      "Step 511: train loss: 0.3178718686103821\n",
      "Step 511: val loss: 0.29236748814582825\n",
      "Step 512: train loss: 0.3170555830001831\n",
      "Step 512: val loss: 0.29154765605926514\n",
      "Step 513: train loss: 0.31624817848205566\n",
      "Step 513: val loss: 0.29073676466941833\n",
      "Step 514: train loss: 0.3154495358467102\n",
      "Step 514: val loss: 0.2899346351623535\n",
      "Step 515: train loss: 0.3146596848964691\n",
      "Step 515: val loss: 0.28914135694503784\n",
      "Step 516: train loss: 0.31387823820114136\n",
      "Step 516: val loss: 0.2883565425872803\n",
      "Step 517: train loss: 0.3131054937839508\n",
      "Step 517: val loss: 0.28758054971694946\n",
      "Step 518: train loss: 0.3123411238193512\n",
      "Step 518: val loss: 0.2868127226829529\n",
      "Step 519: train loss: 0.3115849196910858\n",
      "Step 519: val loss: 0.28605324029922485\n",
      "Step 520: train loss: 0.3108370900154114\n",
      "Step 520: val loss: 0.28530216217041016\n",
      "Step 521: train loss: 0.3100973069667816\n",
      "Step 521: val loss: 0.2845591604709625\n",
      "Step 522: train loss: 0.3093656897544861\n",
      "Step 522: val loss: 0.2838243246078491\n",
      "Step 523: train loss: 0.3086419403553009\n",
      "Step 523: val loss: 0.28309744596481323\n",
      "Step 524: train loss: 0.3079261779785156\n",
      "Step 524: val loss: 0.28237855434417725\n",
      "Step 525: train loss: 0.30721810460090637\n",
      "Step 525: val loss: 0.28166741132736206\n",
      "Step 526: train loss: 0.3065178692340851\n",
      "Step 526: val loss: 0.2809641361236572\n",
      "Step 527: train loss: 0.3058251440525055\n",
      "Step 527: val loss: 0.2802683711051941\n",
      "Step 528: train loss: 0.30513995885849\n",
      "Step 528: val loss: 0.2795802354812622\n",
      "Step 529: train loss: 0.3044623136520386\n",
      "Step 529: val loss: 0.27889958024024963\n",
      "Step 530: train loss: 0.30379199981689453\n",
      "Step 530: val loss: 0.27822619676589966\n",
      "Step 531: train loss: 0.3031289577484131\n",
      "Step 531: val loss: 0.2775602340698242\n",
      "Step 532: train loss: 0.3024730384349823\n",
      "Step 532: val loss: 0.27690163254737854\n",
      "Step 533: train loss: 0.3018244206905365\n",
      "Step 533: val loss: 0.2762501537799835\n",
      "Step 534: train loss: 0.30118274688720703\n",
      "Step 534: val loss: 0.275605708360672\n",
      "Step 535: train loss: 0.30054810643196106\n",
      "Step 535: val loss: 0.27496838569641113\n",
      "Step 536: train loss: 0.29992055892944336\n",
      "Step 536: val loss: 0.27433791756629944\n",
      "Step 537: train loss: 0.2992995083332062\n",
      "Step 537: val loss: 0.2737143933773041\n",
      "Step 538: train loss: 0.2986854612827301\n",
      "Step 538: val loss: 0.2730977535247803\n",
      "Step 539: train loss: 0.2980780303478241\n",
      "Step 539: val loss: 0.272487610578537\n",
      "Step 540: train loss: 0.29747718572616577\n",
      "Step 540: val loss: 0.27188417315483093\n",
      "Step 541: train loss: 0.2968829274177551\n",
      "Step 541: val loss: 0.27128735184669495\n",
      "Step 542: train loss: 0.29629504680633545\n",
      "Step 542: val loss: 0.2706969976425171\n",
      "Step 543: train loss: 0.2957136034965515\n",
      "Step 543: val loss: 0.27011314034461975\n",
      "Step 544: train loss: 0.29513850808143616\n",
      "Step 544: val loss: 0.2695354223251343\n",
      "Step 545: train loss: 0.29456961154937744\n",
      "Step 545: val loss: 0.2689642608165741\n",
      "Step 546: train loss: 0.29400691390037537\n",
      "Step 546: val loss: 0.26839911937713623\n",
      "Step 547: train loss: 0.2934504747390747\n",
      "Step 547: val loss: 0.2678404748439789\n",
      "Step 548: train loss: 0.2928999364376068\n",
      "Step 548: val loss: 0.26728755235671997\n",
      "Step 549: train loss: 0.29235541820526123\n",
      "Step 549: val loss: 0.26674091815948486\n",
      "Step 550: train loss: 0.2918168604373932\n",
      "Step 550: val loss: 0.26620015501976013\n",
      "Step 551: train loss: 0.2912841737270355\n",
      "Step 551: val loss: 0.2656652629375458\n",
      "Step 552: train loss: 0.29075735807418823\n",
      "Step 552: val loss: 0.26513609290122986\n",
      "Step 553: train loss: 0.2902359962463379\n",
      "Step 553: val loss: 0.26461294293403625\n",
      "Step 554: train loss: 0.2897205650806427\n",
      "Step 554: val loss: 0.26409533619880676\n",
      "Step 555: train loss: 0.289210706949234\n",
      "Step 555: val loss: 0.2635834217071533\n",
      "Step 556: train loss: 0.28870636224746704\n",
      "Step 556: val loss: 0.26307716965675354\n",
      "Step 557: train loss: 0.2882075607776642\n",
      "Step 557: val loss: 0.26257628202438354\n",
      "Step 558: train loss: 0.28771406412124634\n",
      "Step 558: val loss: 0.2620808780193329\n",
      "Step 559: train loss: 0.2872259020805359\n",
      "Step 559: val loss: 0.2615910470485687\n",
      "Step 560: train loss: 0.28674325346946716\n",
      "Step 560: val loss: 0.2611064016819\n",
      "Step 561: train loss: 0.2862657904624939\n",
      "Step 561: val loss: 0.2606270909309387\n",
      "Step 562: train loss: 0.28579333424568176\n",
      "Step 562: val loss: 0.26015302538871765\n",
      "Step 563: train loss: 0.28532615303993225\n",
      "Step 563: val loss: 0.25968411564826965\n",
      "Step 564: train loss: 0.28486403822898865\n",
      "Step 564: val loss: 0.25922027230262756\n",
      "Step 565: train loss: 0.28440701961517334\n",
      "Step 565: val loss: 0.2587614953517914\n",
      "Step 566: train loss: 0.28395479917526245\n",
      "Step 566: val loss: 0.2583077549934387\n",
      "Step 567: train loss: 0.28350764513015747\n",
      "Step 567: val loss: 0.25785893201828003\n",
      "Step 568: train loss: 0.2830652892589569\n",
      "Step 568: val loss: 0.2574150562286377\n",
      "Step 569: train loss: 0.28262782096862793\n",
      "Step 569: val loss: 0.25697600841522217\n",
      "Step 570: train loss: 0.28219500184059143\n",
      "Step 570: val loss: 0.2565418481826782\n",
      "Step 571: train loss: 0.28176701068878174\n",
      "Step 571: val loss: 0.2561122179031372\n",
      "Step 572: train loss: 0.28134363889694214\n",
      "Step 572: val loss: 0.25568753480911255\n",
      "Step 573: train loss: 0.28092479705810547\n",
      "Step 573: val loss: 0.25526726245880127\n",
      "Step 574: train loss: 0.2805106043815613\n",
      "Step 574: val loss: 0.2548518180847168\n",
      "Step 575: train loss: 0.28010082244873047\n",
      "Step 575: val loss: 0.25444066524505615\n",
      "Step 576: train loss: 0.2796954810619354\n",
      "Step 576: val loss: 0.2540341317653656\n",
      "Step 577: train loss: 0.2792946398258209\n",
      "Step 577: val loss: 0.2536318302154541\n",
      "Step 578: train loss: 0.27889809012413025\n",
      "Step 578: val loss: 0.2532341778278351\n",
      "Step 579: train loss: 0.278505802154541\n",
      "Step 579: val loss: 0.25284048914909363\n",
      "Step 580: train loss: 0.27811792492866516\n",
      "Step 580: val loss: 0.2524515986442566\n",
      "Step 581: train loss: 0.2777342200279236\n",
      "Step 581: val loss: 0.2520667016506195\n",
      "Step 582: train loss: 0.2773546278476715\n",
      "Step 582: val loss: 0.25168612599372864\n",
      "Step 583: train loss: 0.2769792079925537\n",
      "Step 583: val loss: 0.25130966305732727\n",
      "Step 584: train loss: 0.27660781145095825\n",
      "Step 584: val loss: 0.25093740224838257\n",
      "Step 585: train loss: 0.2762405276298523\n",
      "Step 585: val loss: 0.2505691945552826\n",
      "Step 586: train loss: 0.27587732672691345\n",
      "Step 586: val loss: 0.25020480155944824\n",
      "Step 587: train loss: 0.275518000125885\n",
      "Step 587: val loss: 0.2498445361852646\n",
      "Step 588: train loss: 0.27516239881515503\n",
      "Step 588: val loss: 0.2494880110025406\n",
      "Step 589: train loss: 0.2748108208179474\n",
      "Step 589: val loss: 0.24913568794727325\n",
      "Step 590: train loss: 0.2744629979133606\n",
      "Step 590: val loss: 0.24878688156604767\n",
      "Step 591: train loss: 0.27411898970603943\n",
      "Step 591: val loss: 0.24844211339950562\n",
      "Step 592: train loss: 0.2737787663936615\n",
      "Step 592: val loss: 0.24810123443603516\n",
      "Step 593: train loss: 0.2734421491622925\n",
      "Step 593: val loss: 0.24776388704776764\n",
      "Step 594: train loss: 0.27310922741889954\n",
      "Step 594: val loss: 0.24743011593818665\n",
      "Step 595: train loss: 0.2727799713611603\n",
      "Step 595: val loss: 0.24710041284561157\n",
      "Step 596: train loss: 0.27245429158210754\n",
      "Step 596: val loss: 0.24677394330501556\n",
      "Step 597: train loss: 0.27213212847709656\n",
      "Step 597: val loss: 0.2464512288570404\n",
      "Step 598: train loss: 0.2718135416507721\n",
      "Step 598: val loss: 0.24613185226917267\n",
      "Step 599: train loss: 0.2714982330799103\n",
      "Step 599: val loss: 0.24581627547740936\n",
      "Step 600: train loss: 0.2711865305900574\n",
      "Step 600: val loss: 0.24550390243530273\n",
      "Step 601: train loss: 0.2708781957626343\n",
      "Step 601: val loss: 0.2451951950788498\n",
      "Step 602: train loss: 0.27057310938835144\n",
      "Step 602: val loss: 0.24488964676856995\n",
      "Step 603: train loss: 0.2702714204788208\n",
      "Step 603: val loss: 0.24458754062652588\n",
      "Step 604: train loss: 0.2699729800224304\n",
      "Step 604: val loss: 0.24428866803646088\n",
      "Step 605: train loss: 0.2696777582168579\n",
      "Step 605: val loss: 0.24399308860301971\n",
      "Step 606: train loss: 0.2693859040737152\n",
      "Step 606: val loss: 0.2437007576227188\n",
      "Step 607: train loss: 0.2690969705581665\n",
      "Step 607: val loss: 0.24341163039207458\n",
      "Step 608: train loss: 0.26881125569343567\n",
      "Step 608: val loss: 0.24312564730644226\n",
      "Step 609: train loss: 0.2685287892818451\n",
      "Step 609: val loss: 0.24284282326698303\n",
      "Step 610: train loss: 0.2682492733001709\n",
      "Step 610: val loss: 0.24256302416324615\n",
      "Step 611: train loss: 0.26797282695770264\n",
      "Step 611: val loss: 0.24228639900684357\n",
      "Step 612: train loss: 0.26769933104515076\n",
      "Step 612: val loss: 0.24201272428035736\n",
      "Step 613: train loss: 0.26742884516716003\n",
      "Step 613: val loss: 0.2417420744895935\n",
      "Step 614: train loss: 0.2671613395214081\n",
      "Step 614: val loss: 0.2414744347333908\n",
      "Step 615: train loss: 0.26689663529396057\n",
      "Step 615: val loss: 0.24120958149433136\n",
      "Step 616: train loss: 0.26663485169410706\n",
      "Step 616: val loss: 0.24094776809215546\n",
      "Step 617: train loss: 0.26637595891952515\n",
      "Step 617: val loss: 0.24068892002105713\n",
      "Step 618: train loss: 0.26611995697021484\n",
      "Step 618: val loss: 0.2404327541589737\n",
      "Step 619: train loss: 0.26586657762527466\n",
      "Step 619: val loss: 0.2401793897151947\n",
      "Step 620: train loss: 0.26561596989631653\n",
      "Step 620: val loss: 0.23992879688739777\n",
      "Step 621: train loss: 0.26536810398101807\n",
      "Step 621: val loss: 0.2396809458732605\n",
      "Step 622: train loss: 0.2651229798793793\n",
      "Step 622: val loss: 0.23943595588207245\n",
      "Step 623: train loss: 0.2648804187774658\n",
      "Step 623: val loss: 0.23919332027435303\n",
      "Step 624: train loss: 0.2646406292915344\n",
      "Step 624: val loss: 0.23895366489887238\n",
      "Step 625: train loss: 0.264403373003006\n",
      "Step 625: val loss: 0.23871660232543945\n",
      "Step 626: train loss: 0.2641686499118805\n",
      "Step 626: val loss: 0.2384820580482483\n",
      "Step 627: train loss: 0.2639365494251251\n",
      "Step 627: val loss: 0.23825030028820038\n",
      "Step 628: train loss: 0.26370692253112793\n",
      "Step 628: val loss: 0.2380208522081375\n",
      "Step 629: train loss: 0.2634797990322113\n",
      "Step 629: val loss: 0.23779398202896118\n",
      "Step 630: train loss: 0.26325514912605286\n",
      "Step 630: val loss: 0.23756955564022064\n",
      "Step 631: train loss: 0.2630329728126526\n",
      "Step 631: val loss: 0.23734760284423828\n",
      "Step 632: train loss: 0.2628132700920105\n",
      "Step 632: val loss: 0.23712806403636932\n",
      "Step 633: train loss: 0.2625957727432251\n",
      "Step 633: val loss: 0.23691102862358093\n",
      "Step 634: train loss: 0.26238083839416504\n",
      "Step 634: val loss: 0.23669642210006714\n",
      "Step 635: train loss: 0.2621680498123169\n",
      "Step 635: val loss: 0.236484095454216\n",
      "Step 636: train loss: 0.2619577944278717\n",
      "Step 636: val loss: 0.23627406358718872\n",
      "Step 637: train loss: 0.26174962520599365\n",
      "Step 637: val loss: 0.2360662817955017\n",
      "Step 638: train loss: 0.26154378056526184\n",
      "Step 638: val loss: 0.2358609437942505\n",
      "Step 639: train loss: 0.2613401710987091\n",
      "Step 639: val loss: 0.23565782606601715\n",
      "Step 640: train loss: 0.26113882660865784\n",
      "Step 640: val loss: 0.23545679450035095\n",
      "Step 641: train loss: 0.26093950867652893\n",
      "Step 641: val loss: 0.23525822162628174\n",
      "Step 642: train loss: 0.26074254512786865\n",
      "Step 642: val loss: 0.23506160080432892\n",
      "Step 643: train loss: 0.26054760813713074\n",
      "Step 643: val loss: 0.2348671853542328\n",
      "Step 644: train loss: 0.2603546977043152\n",
      "Step 644: val loss: 0.23467493057250977\n",
      "Step 645: train loss: 0.2601640820503235\n",
      "Step 645: val loss: 0.23448482155799866\n",
      "Step 646: train loss: 0.259975403547287\n",
      "Step 646: val loss: 0.23429660499095917\n",
      "Step 647: train loss: 0.2597888112068176\n",
      "Step 647: val loss: 0.23411071300506592\n",
      "Step 648: train loss: 0.2596043050289154\n",
      "Step 648: val loss: 0.23392663896083832\n",
      "Step 649: train loss: 0.2594217360019684\n",
      "Step 649: val loss: 0.2337447702884674\n",
      "Step 650: train loss: 0.25924116373062134\n",
      "Step 650: val loss: 0.23356489837169647\n",
      "Step 651: train loss: 0.2590625286102295\n",
      "Step 651: val loss: 0.23338693380355835\n",
      "Step 652: train loss: 0.2588858902454376\n",
      "Step 652: val loss: 0.23321090638637543\n",
      "Step 653: train loss: 0.25871121883392334\n",
      "Step 653: val loss: 0.23303699493408203\n",
      "Step 654: train loss: 0.2585383355617523\n",
      "Step 654: val loss: 0.23286470770835876\n",
      "Step 655: train loss: 0.25836724042892456\n",
      "Step 655: val loss: 0.23269443213939667\n",
      "Step 656: train loss: 0.25819823145866394\n",
      "Step 656: val loss: 0.2325262576341629\n",
      "Step 657: train loss: 0.2580309510231018\n",
      "Step 657: val loss: 0.2323596328496933\n",
      "Step 658: train loss: 0.2578655183315277\n",
      "Step 658: val loss: 0.23219512403011322\n",
      "Step 659: train loss: 0.2577017843723297\n",
      "Step 659: val loss: 0.2320321500301361\n",
      "Step 660: train loss: 0.25753989815711975\n",
      "Step 660: val loss: 0.23187100887298584\n",
      "Step 661: train loss: 0.2573797404766083\n",
      "Step 661: val loss: 0.23171177506446838\n",
      "Step 662: train loss: 0.2572213411331177\n",
      "Step 662: val loss: 0.23155423998832703\n",
      "Step 663: train loss: 0.2570647597312927\n",
      "Step 663: val loss: 0.23139838874340057\n",
      "Step 664: train loss: 0.25690972805023193\n",
      "Step 664: val loss: 0.2312442809343338\n",
      "Step 665: train loss: 0.25675642490386963\n",
      "Step 665: val loss: 0.23109199106693268\n",
      "Step 666: train loss: 0.25660479068756104\n",
      "Step 666: val loss: 0.23094111680984497\n",
      "Step 667: train loss: 0.25645482540130615\n",
      "Step 667: val loss: 0.23079219460487366\n",
      "Step 668: train loss: 0.2563064992427826\n",
      "Step 668: val loss: 0.2306446135044098\n",
      "Step 669: train loss: 0.25615981221199036\n",
      "Step 669: val loss: 0.23049907386302948\n",
      "Step 670: train loss: 0.2560145854949951\n",
      "Step 670: val loss: 0.23035477101802826\n",
      "Step 671: train loss: 0.2558710277080536\n",
      "Step 671: val loss: 0.23021221160888672\n",
      "Step 672: train loss: 0.2557291090488434\n",
      "Step 672: val loss: 0.2300710529088974\n",
      "Step 673: train loss: 0.2555886209011078\n",
      "Step 673: val loss: 0.22993169724941254\n",
      "Step 674: train loss: 0.25544965267181396\n",
      "Step 674: val loss: 0.2297937572002411\n",
      "Step 675: train loss: 0.2553121745586395\n",
      "Step 675: val loss: 0.2296573668718338\n",
      "Step 676: train loss: 0.255176305770874\n",
      "Step 676: val loss: 0.22952240705490112\n",
      "Step 677: train loss: 0.25504183769226074\n",
      "Step 677: val loss: 0.22938910126686096\n",
      "Step 678: train loss: 0.2549087405204773\n",
      "Step 678: val loss: 0.22925697267055511\n",
      "Step 679: train loss: 0.2547772526741028\n",
      "Step 679: val loss: 0.22912658751010895\n",
      "Step 680: train loss: 0.2546471953392029\n",
      "Step 680: val loss: 0.2289973497390747\n",
      "Step 681: train loss: 0.2545183002948761\n",
      "Step 681: val loss: 0.22886991500854492\n",
      "Step 682: train loss: 0.254391074180603\n",
      "Step 682: val loss: 0.22874362766742706\n",
      "Step 683: train loss: 0.2542651891708374\n",
      "Step 683: val loss: 0.22861889004707336\n",
      "Step 684: train loss: 0.2541405260562897\n",
      "Step 684: val loss: 0.22849537432193756\n",
      "Step 685: train loss: 0.25401729345321655\n",
      "Step 685: val loss: 0.22837337851524353\n",
      "Step 686: train loss: 0.2538954019546509\n",
      "Step 686: val loss: 0.22825248539447784\n",
      "Step 687: train loss: 0.25377488136291504\n",
      "Step 687: val loss: 0.22813312709331512\n",
      "Step 688: train loss: 0.25365567207336426\n",
      "Step 688: val loss: 0.22801488637924194\n",
      "Step 689: train loss: 0.2535375654697418\n",
      "Step 689: val loss: 0.22789819538593292\n",
      "Step 690: train loss: 0.253420889377594\n",
      "Step 690: val loss: 0.227782741189003\n",
      "Step 691: train loss: 0.2533055543899536\n",
      "Step 691: val loss: 0.2276686131954193\n",
      "Step 692: train loss: 0.25319144129753113\n",
      "Step 692: val loss: 0.22755557298660278\n",
      "Step 693: train loss: 0.2530784606933594\n",
      "Step 693: val loss: 0.2274438887834549\n",
      "Step 694: train loss: 0.2529667615890503\n",
      "Step 694: val loss: 0.2273334115743637\n",
      "Step 695: train loss: 0.2528562843799591\n",
      "Step 695: val loss: 0.2272241711616516\n",
      "Step 696: train loss: 0.2527470290660858\n",
      "Step 696: val loss: 0.227116197347641\n",
      "Step 697: train loss: 0.25263890624046326\n",
      "Step 697: val loss: 0.22700943052768707\n",
      "Step 698: train loss: 0.252532035112381\n",
      "Step 698: val loss: 0.22690364718437195\n",
      "Step 699: train loss: 0.25242626667022705\n",
      "Step 699: val loss: 0.22679919004440308\n",
      "Step 700: train loss: 0.25232166051864624\n",
      "Step 700: val loss: 0.22669610381126404\n",
      "Step 701: train loss: 0.25221821665763855\n",
      "Step 701: val loss: 0.22659391164779663\n",
      "Step 702: train loss: 0.25211581587791443\n",
      "Step 702: val loss: 0.2264927327632904\n",
      "Step 703: train loss: 0.2520146071910858\n",
      "Step 703: val loss: 0.226392924785614\n",
      "Step 704: train loss: 0.25191450119018555\n",
      "Step 704: val loss: 0.22629396617412567\n",
      "Step 705: train loss: 0.25181543827056885\n",
      "Step 705: val loss: 0.22619624435901642\n",
      "Step 706: train loss: 0.2517174184322357\n",
      "Step 706: val loss: 0.22609956562519073\n",
      "Step 707: train loss: 0.2516205608844757\n",
      "Step 707: val loss: 0.22600407898426056\n",
      "Step 708: train loss: 0.25152459740638733\n",
      "Step 708: val loss: 0.22590945661067963\n",
      "Step 709: train loss: 0.2514297664165497\n",
      "Step 709: val loss: 0.2258160412311554\n",
      "Step 710: train loss: 0.25133606791496277\n",
      "Step 710: val loss: 0.22572360932826996\n",
      "Step 711: train loss: 0.2512432336807251\n",
      "Step 711: val loss: 0.2256322056055069\n",
      "Step 712: train loss: 0.25115153193473816\n",
      "Step 712: val loss: 0.22554194927215576\n",
      "Step 713: train loss: 0.25106075406074524\n",
      "Step 713: val loss: 0.22545243799686432\n",
      "Step 714: train loss: 0.25097090005874634\n",
      "Step 714: val loss: 0.22536401450634003\n",
      "Step 715: train loss: 0.25088217854499817\n",
      "Step 715: val loss: 0.2252766489982605\n",
      "Step 716: train loss: 0.2507942318916321\n",
      "Step 716: val loss: 0.22519007325172424\n",
      "Step 717: train loss: 0.2507074177265167\n",
      "Step 717: val loss: 0.22510473430156708\n",
      "Step 718: train loss: 0.25062134861946106\n",
      "Step 718: val loss: 0.22502002120018005\n",
      "Step 719: train loss: 0.25053635239601135\n",
      "Step 719: val loss: 0.22493648529052734\n",
      "Step 720: train loss: 0.25045228004455566\n",
      "Step 720: val loss: 0.22485381364822388\n",
      "Step 721: train loss: 0.250369131565094\n",
      "Step 721: val loss: 0.22477219998836517\n",
      "Step 722: train loss: 0.2502867579460144\n",
      "Step 722: val loss: 0.2246912121772766\n",
      "Step 723: train loss: 0.2502053678035736\n",
      "Step 723: val loss: 0.22461140155792236\n",
      "Step 724: train loss: 0.2501249611377716\n",
      "Step 724: val loss: 0.22453218698501587\n",
      "Step 725: train loss: 0.2500452697277069\n",
      "Step 725: val loss: 0.22445404529571533\n",
      "Step 726: train loss: 0.24996647238731384\n",
      "Step 726: val loss: 0.22437669336795807\n",
      "Step 727: train loss: 0.2498885989189148\n",
      "Step 727: val loss: 0.22430045902729034\n",
      "Step 728: train loss: 0.249811589717865\n",
      "Step 728: val loss: 0.22422470152378082\n",
      "Step 729: train loss: 0.24973535537719727\n",
      "Step 729: val loss: 0.22414976358413696\n",
      "Step 730: train loss: 0.24965988099575043\n",
      "Step 730: val loss: 0.224076047539711\n",
      "Step 731: train loss: 0.24958536028862\n",
      "Step 731: val loss: 0.22400285303592682\n",
      "Step 732: train loss: 0.2495114505290985\n",
      "Step 732: val loss: 0.2239304780960083\n",
      "Step 733: train loss: 0.24943862855434418\n",
      "Step 733: val loss: 0.2238590568304062\n",
      "Step 734: train loss: 0.2493664026260376\n",
      "Step 734: val loss: 0.223788321018219\n",
      "Step 735: train loss: 0.24929498136043549\n",
      "Step 735: val loss: 0.2237185686826706\n",
      "Step 736: train loss: 0.24922436475753784\n",
      "Step 736: val loss: 0.22364944219589233\n",
      "Step 737: train loss: 0.24915452301502228\n",
      "Step 737: val loss: 0.22358101606369019\n",
      "Step 738: train loss: 0.2490854263305664\n",
      "Step 738: val loss: 0.2235133945941925\n",
      "Step 739: train loss: 0.24901710450649261\n",
      "Step 739: val loss: 0.22344662249088287\n",
      "Step 740: train loss: 0.24894948303699493\n",
      "Step 740: val loss: 0.223380446434021\n",
      "Step 741: train loss: 0.24888265132904053\n",
      "Step 741: val loss: 0.22331508994102478\n",
      "Step 742: train loss: 0.24881641566753387\n",
      "Step 742: val loss: 0.2232503741979599\n",
      "Step 743: train loss: 0.24875110387802124\n",
      "Step 743: val loss: 0.22318655252456665\n",
      "Step 744: train loss: 0.24868635833263397\n",
      "Step 744: val loss: 0.22312337160110474\n",
      "Step 745: train loss: 0.24862223863601685\n",
      "Step 745: val loss: 0.223060742020607\n",
      "Step 746: train loss: 0.24855901300907135\n",
      "Step 746: val loss: 0.22299903631210327\n",
      "Step 747: train loss: 0.24849633872509003\n",
      "Step 747: val loss: 0.22293797135353088\n",
      "Step 748: train loss: 0.2484344094991684\n",
      "Step 748: val loss: 0.2228773832321167\n",
      "Step 749: train loss: 0.2483730912208557\n",
      "Step 749: val loss: 0.22281768918037415\n",
      "Step 750: train loss: 0.24831245839595795\n",
      "Step 750: val loss: 0.22275841236114502\n",
      "Step 751: train loss: 0.2482524812221527\n",
      "Step 751: val loss: 0.2227001190185547\n",
      "Step 752: train loss: 0.24819321930408478\n",
      "Step 752: val loss: 0.2226422280073166\n",
      "Step 753: train loss: 0.24813447892665863\n",
      "Step 753: val loss: 0.22258515655994415\n",
      "Step 754: train loss: 0.24807651340961456\n",
      "Step 754: val loss: 0.22252866625785828\n",
      "Step 755: train loss: 0.24801915884017944\n",
      "Step 755: val loss: 0.2224729210138321\n",
      "Step 756: train loss: 0.24796238541603088\n",
      "Step 756: val loss: 0.22241763770580292\n",
      "Step 757: train loss: 0.24790611863136292\n",
      "Step 757: val loss: 0.22236298024654388\n",
      "Step 758: train loss: 0.24785055220127106\n",
      "Step 758: val loss: 0.22230897843837738\n",
      "Step 759: train loss: 0.2477957308292389\n",
      "Step 759: val loss: 0.2222556322813034\n",
      "Step 760: train loss: 0.24774128198623657\n",
      "Step 760: val loss: 0.2222028374671936\n",
      "Step 761: train loss: 0.24768753349781036\n",
      "Step 761: val loss: 0.2221505343914032\n",
      "Step 762: train loss: 0.24763447046279907\n",
      "Step 762: val loss: 0.22209900617599487\n",
      "Step 763: train loss: 0.24758177995681763\n",
      "Step 763: val loss: 0.22204799950122833\n",
      "Step 764: train loss: 0.2475298047065735\n",
      "Step 764: val loss: 0.22199758887290955\n",
      "Step 765: train loss: 0.2474784106016159\n",
      "Step 765: val loss: 0.2219475507736206\n",
      "Step 766: train loss: 0.24742746353149414\n",
      "Step 766: val loss: 0.2218984216451645\n",
      "Step 767: train loss: 0.24737706780433655\n",
      "Step 767: val loss: 0.2218495011329651\n",
      "Step 768: train loss: 0.24732723832130432\n",
      "Step 768: val loss: 0.2218012809753418\n",
      "Step 769: train loss: 0.24727800488471985\n",
      "Step 769: val loss: 0.22175350785255432\n",
      "Step 770: train loss: 0.24722927808761597\n",
      "Step 770: val loss: 0.22170646488666534\n",
      "Step 771: train loss: 0.24718105792999268\n",
      "Step 771: val loss: 0.2216596007347107\n",
      "Step 772: train loss: 0.2471332997083664\n",
      "Step 772: val loss: 0.22161374986171722\n",
      "Step 773: train loss: 0.24708622694015503\n",
      "Step 773: val loss: 0.22156791388988495\n",
      "Step 774: train loss: 0.24703951179981232\n",
      "Step 774: val loss: 0.22152289748191833\n",
      "Step 775: train loss: 0.24699345231056213\n",
      "Step 775: val loss: 0.22147826850414276\n",
      "Step 776: train loss: 0.2469477653503418\n",
      "Step 776: val loss: 0.22143436968326569\n",
      "Step 777: train loss: 0.24690261483192444\n",
      "Step 777: val loss: 0.22139060497283936\n",
      "Step 778: train loss: 0.2468579262495041\n",
      "Step 778: val loss: 0.22134771943092346\n",
      "Step 779: train loss: 0.24681372940540314\n",
      "Step 779: val loss: 0.2213049679994583\n",
      "Step 780: train loss: 0.24677011370658875\n",
      "Step 780: val loss: 0.22126300632953644\n",
      "Step 781: train loss: 0.24672697484493256\n",
      "Step 781: val loss: 0.2212211787700653\n",
      "Step 782: train loss: 0.2466842085123062\n",
      "Step 782: val loss: 0.2211800217628479\n",
      "Step 783: train loss: 0.24664193391799927\n",
      "Step 783: val loss: 0.22113925218582153\n",
      "Step 784: train loss: 0.24660007655620575\n",
      "Step 784: val loss: 0.2210991084575653\n",
      "Step 785: train loss: 0.24655868113040924\n",
      "Step 785: val loss: 0.22105930745601654\n",
      "Step 786: train loss: 0.24651771783828735\n",
      "Step 786: val loss: 0.2210199534893036\n",
      "Step 787: train loss: 0.24647726118564606\n",
      "Step 787: val loss: 0.2209809124469757\n",
      "Step 788: train loss: 0.24643728137016296\n",
      "Step 788: val loss: 0.22094254195690155\n",
      "Step 789: train loss: 0.24639767408370972\n",
      "Step 789: val loss: 0.2209044247865677\n",
      "Step 790: train loss: 0.2463584691286087\n",
      "Step 790: val loss: 0.2208668291568756\n",
      "Step 791: train loss: 0.24631968140602112\n",
      "Step 791: val loss: 0.22082963585853577\n",
      "Step 792: train loss: 0.24628135561943054\n",
      "Step 792: val loss: 0.220792755484581\n",
      "Step 793: train loss: 0.246243417263031\n",
      "Step 793: val loss: 0.22075659036636353\n",
      "Step 794: train loss: 0.24620592594146729\n",
      "Step 794: val loss: 0.22072049975395203\n",
      "Step 795: train loss: 0.2461688369512558\n",
      "Step 795: val loss: 0.2206849604845047\n",
      "Step 796: train loss: 0.24613219499588013\n",
      "Step 796: val loss: 0.2206498384475708\n",
      "Step 797: train loss: 0.24609583616256714\n",
      "Step 797: val loss: 0.22061505913734436\n",
      "Step 798: train loss: 0.24606004357337952\n",
      "Step 798: val loss: 0.2205808013677597\n",
      "Step 799: train loss: 0.2460244745016098\n",
      "Step 799: val loss: 0.22054679691791534\n",
      "Step 800: train loss: 0.24598932266235352\n",
      "Step 800: val loss: 0.22051307559013367\n",
      "Step 801: train loss: 0.24595458805561066\n",
      "Step 801: val loss: 0.22047992050647736\n",
      "Step 802: train loss: 0.24592018127441406\n",
      "Step 802: val loss: 0.22044698894023895\n",
      "Step 803: train loss: 0.24588625133037567\n",
      "Step 803: val loss: 0.22041457891464233\n",
      "Step 804: train loss: 0.2458525151014328\n",
      "Step 804: val loss: 0.22038251161575317\n",
      "Step 805: train loss: 0.2458193153142929\n",
      "Step 805: val loss: 0.22035087645053864\n",
      "Step 806: train loss: 0.24578647315502167\n",
      "Step 806: val loss: 0.2203194797039032\n",
      "Step 807: train loss: 0.2457539588212967\n",
      "Step 807: val loss: 0.22028833627700806\n",
      "Step 808: train loss: 0.24572166800498962\n",
      "Step 808: val loss: 0.22025762498378754\n",
      "Step 809: train loss: 0.24568979442119598\n",
      "Step 809: val loss: 0.22022733092308044\n",
      "Step 810: train loss: 0.24565832316875458\n",
      "Step 810: val loss: 0.2201974093914032\n",
      "Step 811: train loss: 0.24562720954418182\n",
      "Step 811: val loss: 0.22016775608062744\n",
      "Step 812: train loss: 0.24559642374515533\n",
      "Step 812: val loss: 0.22013846039772034\n",
      "Step 813: train loss: 0.24556589126586914\n",
      "Step 813: val loss: 0.22010941803455353\n",
      "Step 814: train loss: 0.2455357164144516\n",
      "Step 814: val loss: 0.2200809121131897\n",
      "Step 815: train loss: 0.2455059289932251\n",
      "Step 815: val loss: 0.22005245089530945\n",
      "Step 816: train loss: 0.2454763799905777\n",
      "Step 816: val loss: 0.2200247198343277\n",
      "Step 817: train loss: 0.24544723331928253\n",
      "Step 817: val loss: 0.21999691426753998\n",
      "Step 818: train loss: 0.24541832506656647\n",
      "Step 818: val loss: 0.21996963024139404\n",
      "Step 819: train loss: 0.24538984894752502\n",
      "Step 819: val loss: 0.2199423760175705\n",
      "Step 820: train loss: 0.2453615516424179\n",
      "Step 820: val loss: 0.21991565823554993\n",
      "Step 821: train loss: 0.24533353745937347\n",
      "Step 821: val loss: 0.2198893129825592\n",
      "Step 822: train loss: 0.24530605971813202\n",
      "Step 822: val loss: 0.21986311674118042\n",
      "Step 823: train loss: 0.24527865648269653\n",
      "Step 823: val loss: 0.2198372781276703\n",
      "Step 824: train loss: 0.2452516108751297\n",
      "Step 824: val loss: 0.21981163322925568\n",
      "Step 825: train loss: 0.24522489309310913\n",
      "Step 825: val loss: 0.21978642046451569\n",
      "Step 826: train loss: 0.24519847333431244\n",
      "Step 826: val loss: 0.21976152062416077\n",
      "Step 827: train loss: 0.24517221748828888\n",
      "Step 827: val loss: 0.21973693370819092\n",
      "Step 828: train loss: 0.24514639377593994\n",
      "Step 828: val loss: 0.2197125107049942\n",
      "Step 829: train loss: 0.2451207935810089\n",
      "Step 829: val loss: 0.21968835592269897\n",
      "Step 830: train loss: 0.24509555101394653\n",
      "Step 830: val loss: 0.21966452896595\n",
      "Step 831: train loss: 0.2450704425573349\n",
      "Step 831: val loss: 0.21964091062545776\n",
      "Step 832: train loss: 0.2450457066297531\n",
      "Step 832: val loss: 0.2196175754070282\n",
      "Step 833: train loss: 0.24502111971378326\n",
      "Step 833: val loss: 0.21959461271762848\n",
      "Step 834: train loss: 0.24499690532684326\n",
      "Step 834: val loss: 0.21957166492938995\n",
      "Step 835: train loss: 0.24497291445732117\n",
      "Step 835: val loss: 0.2195492684841156\n",
      "Step 836: train loss: 0.24494916200637817\n",
      "Step 836: val loss: 0.21952705085277557\n",
      "Step 837: train loss: 0.24492576718330383\n",
      "Step 837: val loss: 0.21950498223304749\n",
      "Step 838: train loss: 0.24490249156951904\n",
      "Step 838: val loss: 0.21948322653770447\n",
      "Step 839: train loss: 0.24487949907779694\n",
      "Step 839: val loss: 0.21946179866790771\n",
      "Step 840: train loss: 0.24485689401626587\n",
      "Step 840: val loss: 0.21944057941436768\n",
      "Step 841: train loss: 0.24483437836170197\n",
      "Step 841: val loss: 0.21941940486431122\n",
      "Step 842: train loss: 0.2448122799396515\n",
      "Step 842: val loss: 0.219398632645607\n",
      "Step 843: train loss: 0.24479028582572937\n",
      "Step 843: val loss: 0.21937817335128784\n",
      "Step 844: train loss: 0.24476850032806396\n",
      "Step 844: val loss: 0.21935784816741943\n",
      "Step 845: train loss: 0.24474696815013885\n",
      "Step 845: val loss: 0.2193378061056137\n",
      "Step 846: train loss: 0.24472574889659882\n",
      "Step 846: val loss: 0.21931779384613037\n",
      "Step 847: train loss: 0.24470467865467072\n",
      "Step 847: val loss: 0.21929843723773956\n",
      "Step 848: train loss: 0.2446838766336441\n",
      "Step 848: val loss: 0.21927869319915771\n",
      "Step 849: train loss: 0.24466335773468018\n",
      "Step 849: val loss: 0.21925979852676392\n",
      "Step 850: train loss: 0.24464300274848938\n",
      "Step 850: val loss: 0.2192407250404358\n",
      "Step 851: train loss: 0.24462276697158813\n",
      "Step 851: val loss: 0.21922200918197632\n",
      "Step 852: train loss: 0.24460279941558838\n",
      "Step 852: val loss: 0.21920348703861237\n",
      "Step 853: train loss: 0.24458318948745728\n",
      "Step 853: val loss: 0.2191852182149887\n",
      "Step 854: train loss: 0.24456371366977692\n",
      "Step 854: val loss: 0.21916714310646057\n",
      "Step 855: train loss: 0.24454450607299805\n",
      "Step 855: val loss: 0.21914926171302795\n",
      "Step 856: train loss: 0.24452531337738037\n",
      "Step 856: val loss: 0.2191316783428192\n",
      "Step 857: train loss: 0.24450650811195374\n",
      "Step 857: val loss: 0.2191140204668045\n",
      "Step 858: train loss: 0.24448782205581665\n",
      "Step 858: val loss: 0.21909692883491516\n",
      "Step 859: train loss: 0.24446941912174225\n",
      "Step 859: val loss: 0.21907974779605865\n",
      "Step 860: train loss: 0.244451105594635\n",
      "Step 860: val loss: 0.21906284987926483\n",
      "Step 861: train loss: 0.2444329857826233\n",
      "Step 861: val loss: 0.2190462052822113\n",
      "Step 862: train loss: 0.24441519379615784\n",
      "Step 862: val loss: 0.2190297544002533\n",
      "Step 863: train loss: 0.24439756572246552\n",
      "Step 863: val loss: 0.21901342272758484\n",
      "Step 864: train loss: 0.24438007175922394\n",
      "Step 864: val loss: 0.21899734437465668\n",
      "Step 865: train loss: 0.2443627566099167\n",
      "Step 865: val loss: 0.21898148953914642\n",
      "Step 866: train loss: 0.24434566497802734\n",
      "Step 866: val loss: 0.21896575391292572\n",
      "Step 867: train loss: 0.24432867765426636\n",
      "Step 867: val loss: 0.21895015239715576\n",
      "Step 868: train loss: 0.24431192874908447\n",
      "Step 868: val loss: 0.21893467009067535\n",
      "Step 869: train loss: 0.24429547786712646\n",
      "Step 869: val loss: 0.21891945600509644\n",
      "Step 870: train loss: 0.24427908658981323\n",
      "Step 870: val loss: 0.2189043164253235\n",
      "Step 871: train loss: 0.2442629486322403\n",
      "Step 871: val loss: 0.21888957917690277\n",
      "Step 872: train loss: 0.24424690008163452\n",
      "Step 872: val loss: 0.2188747078180313\n",
      "Step 873: train loss: 0.24423100054264069\n",
      "Step 873: val loss: 0.2188602089881897\n",
      "Step 874: train loss: 0.24421536922454834\n",
      "Step 874: val loss: 0.21884582936763763\n",
      "Step 875: train loss: 0.24419988691806793\n",
      "Step 875: val loss: 0.21883147954940796\n",
      "Step 876: train loss: 0.24418455362319946\n",
      "Step 876: val loss: 0.2188175618648529\n",
      "Step 877: train loss: 0.24416932463645935\n",
      "Step 877: val loss: 0.21880362927913666\n",
      "Step 878: train loss: 0.24415433406829834\n",
      "Step 878: val loss: 0.2187899351119995\n",
      "Step 879: train loss: 0.24413952231407166\n",
      "Step 879: val loss: 0.21877644956111908\n",
      "Step 880: train loss: 0.2441249042749405\n",
      "Step 880: val loss: 0.21876312792301178\n",
      "Step 881: train loss: 0.2441103309392929\n",
      "Step 881: val loss: 0.21875\n",
      "Step 882: train loss: 0.2440960556268692\n",
      "Step 882: val loss: 0.218736931681633\n",
      "Step 883: train loss: 0.2440817654132843\n",
      "Step 883: val loss: 0.2187240570783615\n",
      "Step 884: train loss: 0.2440677136182785\n",
      "Step 884: val loss: 0.21871119737625122\n",
      "Step 885: train loss: 0.24405384063720703\n",
      "Step 885: val loss: 0.21869857609272003\n",
      "Step 886: train loss: 0.24404005706310272\n",
      "Step 886: val loss: 0.2186863273382187\n",
      "Step 887: train loss: 0.24402648210525513\n",
      "Step 887: val loss: 0.218673974275589\n",
      "Step 888: train loss: 0.24401302635669708\n",
      "Step 888: val loss: 0.21866165101528168\n",
      "Step 889: train loss: 0.2439996898174286\n",
      "Step 889: val loss: 0.21864968538284302\n",
      "Step 890: train loss: 0.2439866065979004\n",
      "Step 890: val loss: 0.21863768994808197\n",
      "Step 891: train loss: 0.24397359788417816\n",
      "Step 891: val loss: 0.2186262458562851\n",
      "Step 892: train loss: 0.24396070837974548\n",
      "Step 892: val loss: 0.2186143696308136\n",
      "Step 893: train loss: 0.24394796788692474\n",
      "Step 893: val loss: 0.2186030149459839\n",
      "Step 894: train loss: 0.24393531680107117\n",
      "Step 894: val loss: 0.21859140694141388\n",
      "Step 895: train loss: 0.2439229041337967\n",
      "Step 895: val loss: 0.21858051419258118\n",
      "Step 896: train loss: 0.2439105212688446\n",
      "Step 896: val loss: 0.21856927871704102\n",
      "Step 897: train loss: 0.2438983917236328\n",
      "Step 897: val loss: 0.21855837106704712\n",
      "Step 898: train loss: 0.24388626217842102\n",
      "Step 898: val loss: 0.21854761242866516\n",
      "Step 899: train loss: 0.24387440085411072\n",
      "Step 899: val loss: 0.21853692829608917\n",
      "Step 900: train loss: 0.24386249482631683\n",
      "Step 900: val loss: 0.21852630376815796\n",
      "Step 901: train loss: 0.24385085701942444\n",
      "Step 901: val loss: 0.2185157984495163\n",
      "Step 902: train loss: 0.2438393086194992\n",
      "Step 902: val loss: 0.2185056358575821\n",
      "Step 903: train loss: 0.24382789433002472\n",
      "Step 903: val loss: 0.21849529445171356\n",
      "Step 904: train loss: 0.24381659924983978\n",
      "Step 904: val loss: 0.21848520636558533\n",
      "Step 905: train loss: 0.24380548298358917\n",
      "Step 905: val loss: 0.218475341796875\n",
      "Step 906: train loss: 0.24379436671733856\n",
      "Step 906: val loss: 0.21846544742584229\n",
      "Step 907: train loss: 0.24378345906734467\n",
      "Step 907: val loss: 0.2184557467699051\n",
      "Step 908: train loss: 0.2437726855278015\n",
      "Step 908: val loss: 0.21844597160816193\n",
      "Step 909: train loss: 0.24376197159290314\n",
      "Step 909: val loss: 0.21843667328357697\n",
      "Step 910: train loss: 0.2437513768672943\n",
      "Step 910: val loss: 0.21842709183692932\n",
      "Step 911: train loss: 0.24374085664749146\n",
      "Step 911: val loss: 0.2184179425239563\n",
      "Step 912: train loss: 0.24373066425323486\n",
      "Step 912: val loss: 0.2184087634086609\n",
      "Step 913: train loss: 0.24372035264968872\n",
      "Step 913: val loss: 0.21839965879917145\n",
      "Step 914: train loss: 0.24371013045310974\n",
      "Step 914: val loss: 0.21839067339897156\n",
      "Step 915: train loss: 0.24370022118091583\n",
      "Step 915: val loss: 0.2183818370103836\n",
      "Step 916: train loss: 0.24369023740291595\n",
      "Step 916: val loss: 0.21837307512760162\n",
      "Step 917: train loss: 0.24368040263652802\n",
      "Step 917: val loss: 0.21836446225643158\n",
      "Step 918: train loss: 0.2436707317829132\n",
      "Step 918: val loss: 0.2183559238910675\n",
      "Step 919: train loss: 0.24366119503974915\n",
      "Step 919: val loss: 0.21834756433963776\n",
      "Step 920: train loss: 0.24365167319774628\n",
      "Step 920: val loss: 0.21833905577659607\n",
      "Step 921: train loss: 0.24364227056503296\n",
      "Step 921: val loss: 0.21833090484142303\n",
      "Step 922: train loss: 0.24363306164741516\n",
      "Step 922: val loss: 0.21832288801670074\n",
      "Step 923: train loss: 0.24362380802631378\n",
      "Step 923: val loss: 0.21831460297107697\n",
      "Step 924: train loss: 0.2436147928237915\n",
      "Step 924: val loss: 0.21830666065216064\n",
      "Step 925: train loss: 0.24360573291778564\n",
      "Step 925: val loss: 0.21829892694950104\n",
      "Step 926: train loss: 0.2435969114303589\n",
      "Step 926: val loss: 0.21829120814800262\n",
      "Step 927: train loss: 0.24358811974525452\n",
      "Step 927: val loss: 0.2182835042476654\n",
      "Step 928: train loss: 0.2435794621706009\n",
      "Step 928: val loss: 0.21827581524848938\n",
      "Step 929: train loss: 0.24357084929943085\n",
      "Step 929: val loss: 0.21826837956905365\n",
      "Step 930: train loss: 0.24356231093406677\n",
      "Step 930: val loss: 0.2182610183954239\n",
      "Step 931: train loss: 0.24355386197566986\n",
      "Step 931: val loss: 0.21825367212295532\n",
      "Step 932: train loss: 0.2435455471277237\n",
      "Step 932: val loss: 0.21824651956558228\n",
      "Step 933: train loss: 0.24353735148906708\n",
      "Step 933: val loss: 0.21823939681053162\n",
      "Step 934: train loss: 0.24352914094924927\n",
      "Step 934: val loss: 0.21823230385780334\n",
      "Step 935: train loss: 0.2435210943222046\n",
      "Step 935: val loss: 0.21822533011436462\n",
      "Step 936: train loss: 0.24351319670677185\n",
      "Step 936: val loss: 0.21821855008602142\n",
      "Step 937: train loss: 0.24350526928901672\n",
      "Step 937: val loss: 0.2182115614414215\n",
      "Step 938: train loss: 0.2434975653886795\n",
      "Step 938: val loss: 0.218205064535141\n",
      "Step 939: train loss: 0.24348975718021393\n",
      "Step 939: val loss: 0.21819816529750824\n",
      "Step 940: train loss: 0.24348220229148865\n",
      "Step 940: val loss: 0.21819201111793518\n",
      "Step 941: train loss: 0.24347463250160217\n",
      "Step 941: val loss: 0.21818512678146362\n",
      "Step 942: train loss: 0.24346715211868286\n",
      "Step 942: val loss: 0.21817922592163086\n",
      "Step 943: train loss: 0.24345985054969788\n",
      "Step 943: val loss: 0.21817263960838318\n",
      "Step 944: train loss: 0.2434525042772293\n",
      "Step 944: val loss: 0.21816657483577728\n",
      "Step 945: train loss: 0.2434452772140503\n",
      "Step 945: val loss: 0.21816033124923706\n",
      "Step 946: train loss: 0.24343810975551605\n",
      "Step 946: val loss: 0.21815423667430878\n",
      "Step 947: train loss: 0.24343103170394897\n",
      "Step 947: val loss: 0.21814832091331482\n",
      "Step 948: train loss: 0.24342402815818787\n",
      "Step 948: val loss: 0.2181423157453537\n",
      "Step 949: train loss: 0.2434171438217163\n",
      "Step 949: val loss: 0.21813666820526123\n",
      "Step 950: train loss: 0.24341028928756714\n",
      "Step 950: val loss: 0.21813073754310608\n",
      "Step 951: train loss: 0.24340352416038513\n",
      "Step 951: val loss: 0.2181250900030136\n",
      "Step 952: train loss: 0.2433968335390091\n",
      "Step 952: val loss: 0.21811939775943756\n",
      "Step 953: train loss: 0.24339020252227783\n",
      "Step 953: val loss: 0.2181137055158615\n",
      "Step 954: train loss: 0.24338366091251373\n",
      "Step 954: val loss: 0.21810829639434814\n",
      "Step 955: train loss: 0.24337714910507202\n",
      "Step 955: val loss: 0.21810272336006165\n",
      "Step 956: train loss: 0.24337081611156464\n",
      "Step 956: val loss: 0.21809729933738708\n",
      "Step 957: train loss: 0.24336445331573486\n",
      "Step 957: val loss: 0.2180919498205185\n",
      "Step 958: train loss: 0.24335819482803345\n",
      "Step 958: val loss: 0.21808676421642303\n",
      "Step 959: train loss: 0.2433519810438156\n",
      "Step 959: val loss: 0.21808142960071564\n",
      "Step 960: train loss: 0.24334578216075897\n",
      "Step 960: val loss: 0.21807631850242615\n",
      "Step 961: train loss: 0.24333983659744263\n",
      "Step 961: val loss: 0.21807125210762024\n",
      "Step 962: train loss: 0.24333377182483673\n",
      "Step 962: val loss: 0.21806634962558746\n",
      "Step 963: train loss: 0.24332784116268158\n",
      "Step 963: val loss: 0.21806137263774872\n",
      "Step 964: train loss: 0.2433219850063324\n",
      "Step 964: val loss: 0.21805648505687714\n",
      "Step 965: train loss: 0.24331612884998322\n",
      "Step 965: val loss: 0.2180517166852951\n",
      "Step 966: train loss: 0.2433103770017624\n",
      "Step 966: val loss: 0.21804670989513397\n",
      "Step 967: train loss: 0.24330474436283112\n",
      "Step 967: val loss: 0.21804209053516388\n",
      "Step 968: train loss: 0.24329914152622223\n",
      "Step 968: val loss: 0.21803738176822662\n",
      "Step 969: train loss: 0.2432936429977417\n",
      "Step 969: val loss: 0.21803292632102966\n",
      "Step 970: train loss: 0.2432880997657776\n",
      "Step 970: val loss: 0.21802832186222076\n",
      "Step 971: train loss: 0.24328264594078064\n",
      "Step 971: val loss: 0.21802391111850739\n",
      "Step 972: train loss: 0.24327722191810608\n",
      "Step 972: val loss: 0.2180192768573761\n",
      "Step 973: train loss: 0.24327188730239868\n",
      "Step 973: val loss: 0.21801510453224182\n",
      "Step 974: train loss: 0.24326664209365845\n",
      "Step 974: val loss: 0.2180105596780777\n",
      "Step 975: train loss: 0.24326133728027344\n",
      "Step 975: val loss: 0.21800652146339417\n",
      "Step 976: train loss: 0.2432563602924347\n",
      "Step 976: val loss: 0.21800203621387482\n",
      "Step 977: train loss: 0.24325118958950043\n",
      "Step 977: val loss: 0.21799816191196442\n",
      "Step 978: train loss: 0.24324621260166168\n",
      "Step 978: val loss: 0.21799379587173462\n",
      "Step 979: train loss: 0.24324117600917816\n",
      "Step 979: val loss: 0.2179899662733078\n",
      "Step 980: train loss: 0.24323628842830658\n",
      "Step 980: val loss: 0.21798568964004517\n",
      "Step 981: train loss: 0.24323131144046783\n",
      "Step 981: val loss: 0.21798187494277954\n",
      "Step 982: train loss: 0.2432265430688858\n",
      "Step 982: val loss: 0.21797792613506317\n",
      "Step 983: train loss: 0.24322175979614258\n",
      "Step 983: val loss: 0.21797405183315277\n",
      "Step 984: train loss: 0.24321705102920532\n",
      "Step 984: val loss: 0.21797028183937073\n",
      "Step 985: train loss: 0.24321235716342926\n",
      "Step 985: val loss: 0.21796655654907227\n",
      "Step 986: train loss: 0.24320779740810394\n",
      "Step 986: val loss: 0.21796274185180664\n",
      "Step 987: train loss: 0.24320311844348907\n",
      "Step 987: val loss: 0.21795913577079773\n",
      "Step 988: train loss: 0.24319864809513092\n",
      "Step 988: val loss: 0.21795539557933807\n",
      "Step 989: train loss: 0.24319419264793396\n",
      "Step 989: val loss: 0.21795205771923065\n",
      "Step 990: train loss: 0.243189737200737\n",
      "Step 990: val loss: 0.21794836223125458\n",
      "Step 991: train loss: 0.2431854009628296\n",
      "Step 991: val loss: 0.21794502437114716\n",
      "Step 992: train loss: 0.24318110942840576\n",
      "Step 992: val loss: 0.21794141829013824\n",
      "Step 993: train loss: 0.24317680299282074\n",
      "Step 993: val loss: 0.21793802082538605\n",
      "Step 994: train loss: 0.2431725710630417\n",
      "Step 994: val loss: 0.2179347574710846\n",
      "Step 995: train loss: 0.243168443441391\n",
      "Step 995: val loss: 0.21793119609355927\n",
      "Step 996: train loss: 0.24316425621509552\n",
      "Step 996: val loss: 0.21792815625667572\n",
      "Step 997: train loss: 0.2431601732969284\n",
      "Step 997: val loss: 0.21792484819889069\n",
      "Step 998: train loss: 0.24315613508224487\n",
      "Step 998: val loss: 0.21792173385620117\n",
      "Step 999: train loss: 0.24315212666988373\n",
      "Step 999: val loss: 0.2179185152053833\n",
      "Step 1000: train loss: 0.2431481033563614\n",
      "Step 1000: val loss: 0.21791549026966095\n",
      "Step 1001: train loss: 0.2431442141532898\n",
      "Step 1001: val loss: 0.2179122418165207\n",
      "Step 1002: train loss: 0.24314041435718536\n",
      "Step 1002: val loss: 0.21790942549705505\n",
      "Step 1003: train loss: 0.243136465549469\n",
      "Step 1003: val loss: 0.21790629625320435\n",
      "Step 1004: train loss: 0.24313265085220337\n",
      "Step 1004: val loss: 0.21790342032909393\n",
      "Step 1005: train loss: 0.2431289553642273\n",
      "Step 1005: val loss: 0.2179003655910492\n",
      "Step 1006: train loss: 0.24312523007392883\n",
      "Step 1006: val loss: 0.21789783239364624\n",
      "Step 1007: train loss: 0.24312154948711395\n",
      "Step 1007: val loss: 0.21789462864398956\n",
      "Step 1008: train loss: 0.2431178092956543\n",
      "Step 1008: val loss: 0.21789208054542542\n",
      "Step 1009: train loss: 0.24311432242393494\n",
      "Step 1009: val loss: 0.21788932383060455\n",
      "Step 1010: train loss: 0.24311070144176483\n",
      "Step 1010: val loss: 0.2178865671157837\n",
      "Step 1011: train loss: 0.24310722947120667\n",
      "Step 1011: val loss: 0.2178838700056076\n",
      "Step 1012: train loss: 0.2431037724018097\n",
      "Step 1012: val loss: 0.21788105368614197\n",
      "Step 1013: train loss: 0.2431003600358963\n",
      "Step 1013: val loss: 0.217878520488739\n",
      "Step 1014: train loss: 0.2430969476699829\n",
      "Step 1014: val loss: 0.21787583827972412\n",
      "Step 1015: train loss: 0.24309344589710236\n",
      "Step 1015: val loss: 0.21787329018115997\n",
      "Step 1016: train loss: 0.24309022724628448\n",
      "Step 1016: val loss: 0.21787063777446747\n",
      "Step 1017: train loss: 0.24308696389198303\n",
      "Step 1017: val loss: 0.21786828339099884\n",
      "Step 1018: train loss: 0.2430836707353592\n",
      "Step 1018: val loss: 0.2178657501935959\n",
      "Step 1019: train loss: 0.24308036267757416\n",
      "Step 1019: val loss: 0.2178632616996765\n",
      "Step 1020: train loss: 0.24307723343372345\n",
      "Step 1020: val loss: 0.21786099672317505\n",
      "Step 1021: train loss: 0.24307411909103394\n",
      "Step 1021: val loss: 0.21785849332809448\n",
      "Step 1022: train loss: 0.24307101964950562\n",
      "Step 1022: val loss: 0.21785618364810944\n",
      "Step 1023: train loss: 0.2430679351091385\n",
      "Step 1023: val loss: 0.21785396337509155\n",
      "Step 1024: train loss: 0.24306489527225494\n",
      "Step 1024: val loss: 0.21785151958465576\n",
      "Step 1025: train loss: 0.24306190013885498\n",
      "Step 1025: val loss: 0.21784929931163788\n",
      "Step 1026: train loss: 0.24305887520313263\n",
      "Step 1026: val loss: 0.21784716844558716\n",
      "Step 1027: train loss: 0.24305588006973267\n",
      "Step 1027: val loss: 0.21784496307373047\n",
      "Step 1028: train loss: 0.24305300414562225\n",
      "Step 1028: val loss: 0.21784259378910065\n",
      "Step 1029: train loss: 0.24305005371570587\n",
      "Step 1029: val loss: 0.21784067153930664\n",
      "Step 1030: train loss: 0.243047297000885\n",
      "Step 1030: val loss: 0.2178383767604828\n",
      "Step 1031: train loss: 0.2430444359779358\n",
      "Step 1031: val loss: 0.21783649921417236\n",
      "Step 1032: train loss: 0.24304160475730896\n",
      "Step 1032: val loss: 0.21783433854579926\n",
      "Step 1033: train loss: 0.2430388480424881\n",
      "Step 1033: val loss: 0.21783237159252167\n",
      "Step 1034: train loss: 0.24303607642650604\n",
      "Step 1034: val loss: 0.2178303599357605\n",
      "Step 1035: train loss: 0.24303340911865234\n",
      "Step 1035: val loss: 0.21782845258712769\n",
      "Step 1036: train loss: 0.24303074181079865\n",
      "Step 1036: val loss: 0.21782627701759338\n",
      "Step 1037: train loss: 0.2430281788110733\n",
      "Step 1037: val loss: 0.21782463788986206\n",
      "Step 1038: train loss: 0.24302542209625244\n",
      "Step 1038: val loss: 0.2178226113319397\n",
      "Step 1039: train loss: 0.2430228441953659\n",
      "Step 1039: val loss: 0.21782079339027405\n",
      "Step 1040: train loss: 0.24302035570144653\n",
      "Step 1040: val loss: 0.21781882643699646\n",
      "Step 1041: train loss: 0.2430177479982376\n",
      "Step 1041: val loss: 0.2178173065185547\n",
      "Step 1042: train loss: 0.24301527440547943\n",
      "Step 1042: val loss: 0.21781505644321442\n",
      "Step 1043: train loss: 0.24301281571388245\n",
      "Step 1043: val loss: 0.21781356632709503\n",
      "Step 1044: train loss: 0.24301032721996307\n",
      "Step 1044: val loss: 0.21781179308891296\n",
      "Step 1045: train loss: 0.24300792813301086\n",
      "Step 1045: val loss: 0.21781006455421448\n",
      "Step 1046: train loss: 0.24300552904605865\n",
      "Step 1046: val loss: 0.2178083062171936\n",
      "Step 1047: train loss: 0.24300304055213928\n",
      "Step 1047: val loss: 0.21780669689178467\n",
      "Step 1048: train loss: 0.24300068616867065\n",
      "Step 1048: val loss: 0.21780504286289215\n",
      "Step 1049: train loss: 0.2429984211921692\n",
      "Step 1049: val loss: 0.21780332922935486\n",
      "Step 1050: train loss: 0.24299614131450653\n",
      "Step 1050: val loss: 0.21780161559581757\n",
      "Step 1051: train loss: 0.24299389123916626\n",
      "Step 1051: val loss: 0.21780018508434296\n",
      "Step 1052: train loss: 0.242991641163826\n",
      "Step 1052: val loss: 0.21779851615428925\n",
      "Step 1053: train loss: 0.24298936128616333\n",
      "Step 1053: val loss: 0.21779705584049225\n",
      "Step 1054: train loss: 0.2429872304201126\n",
      "Step 1054: val loss: 0.2177954465150833\n",
      "Step 1055: train loss: 0.2429850548505783\n",
      "Step 1055: val loss: 0.2177940309047699\n",
      "Step 1056: train loss: 0.24298293888568878\n",
      "Step 1056: val loss: 0.2177923619747162\n",
      "Step 1057: train loss: 0.24298080801963806\n",
      "Step 1057: val loss: 0.21779106557369232\n",
      "Step 1058: train loss: 0.24297870695590973\n",
      "Step 1058: val loss: 0.217789426445961\n",
      "Step 1059: train loss: 0.24297653138637543\n",
      "Step 1059: val loss: 0.21778810024261475\n",
      "Step 1060: train loss: 0.24297450482845306\n",
      "Step 1060: val loss: 0.21778656542301178\n",
      "Step 1061: train loss: 0.24297252297401428\n",
      "Step 1061: val loss: 0.21778526902198792\n",
      "Step 1062: train loss: 0.24297045171260834\n",
      "Step 1062: val loss: 0.21778380870819092\n",
      "Step 1063: train loss: 0.24296844005584717\n",
      "Step 1063: val loss: 0.21778251230716705\n",
      "Step 1064: train loss: 0.24296653270721436\n",
      "Step 1064: val loss: 0.21778112649917603\n",
      "Step 1065: train loss: 0.2429644912481308\n",
      "Step 1065: val loss: 0.21777988970279694\n",
      "Step 1066: train loss: 0.24296262860298157\n",
      "Step 1066: val loss: 0.2177785485982895\n",
      "Step 1067: train loss: 0.24296073615550995\n",
      "Step 1067: val loss: 0.2177773267030716\n",
      "Step 1068: train loss: 0.24295884370803833\n",
      "Step 1068: val loss: 0.21777617931365967\n",
      "Step 1069: train loss: 0.24295693635940552\n",
      "Step 1069: val loss: 0.21777480840682983\n",
      "Step 1070: train loss: 0.24295514822006226\n",
      "Step 1070: val loss: 0.21777379512786865\n",
      "Step 1071: train loss: 0.2429533153772354\n",
      "Step 1071: val loss: 0.21777255833148956\n",
      "Step 1072: train loss: 0.24295148253440857\n",
      "Step 1072: val loss: 0.21777139604091644\n",
      "Step 1073: train loss: 0.2429496943950653\n",
      "Step 1073: val loss: 0.2177702933549881\n",
      "Step 1074: train loss: 0.24294796586036682\n",
      "Step 1074: val loss: 0.2177690863609314\n",
      "Step 1075: train loss: 0.24294613301753998\n",
      "Step 1075: val loss: 0.217768132686615\n",
      "Step 1076: train loss: 0.24294446408748627\n",
      "Step 1076: val loss: 0.2177668809890747\n",
      "Step 1077: train loss: 0.24294281005859375\n",
      "Step 1077: val loss: 0.2177656888961792\n",
      "Step 1078: train loss: 0.24294105172157288\n",
      "Step 1078: val loss: 0.2177647203207016\n",
      "Step 1079: train loss: 0.24293941259384155\n",
      "Step 1079: val loss: 0.21776343882083893\n",
      "Step 1080: train loss: 0.24293765425682068\n",
      "Step 1080: val loss: 0.21776264905929565\n",
      "Step 1081: train loss: 0.2429361343383789\n",
      "Step 1081: val loss: 0.21776118874549866\n",
      "Step 1082: train loss: 0.2429344803094864\n",
      "Step 1082: val loss: 0.21776047348976135\n",
      "Step 1083: train loss: 0.24293281137943268\n",
      "Step 1083: val loss: 0.21775934100151062\n",
      "Step 1084: train loss: 0.2429312914609909\n",
      "Step 1084: val loss: 0.2177584022283554\n",
      "Step 1085: train loss: 0.242929607629776\n",
      "Step 1085: val loss: 0.21775734424591064\n",
      "Step 1086: train loss: 0.24292810261249542\n",
      "Step 1086: val loss: 0.2177564650774002\n",
      "Step 1087: train loss: 0.24292661249637604\n",
      "Step 1087: val loss: 0.21775533258914948\n",
      "Step 1088: train loss: 0.24292515218257904\n",
      "Step 1088: val loss: 0.21775461733341217\n",
      "Step 1089: train loss: 0.2429235577583313\n",
      "Step 1089: val loss: 0.21775341033935547\n",
      "Step 1090: train loss: 0.2429221123456955\n",
      "Step 1090: val loss: 0.21775281429290771\n",
      "Step 1091: train loss: 0.24292057752609253\n",
      "Step 1091: val loss: 0.2177516520023346\n",
      "Step 1092: train loss: 0.2429191619157791\n",
      "Step 1092: val loss: 0.21775087714195251\n",
      "Step 1093: train loss: 0.2429177165031433\n",
      "Step 1093: val loss: 0.21774989366531372\n",
      "Step 1094: train loss: 0.24291633069515228\n",
      "Step 1094: val loss: 0.21774907410144806\n",
      "Step 1095: train loss: 0.24291494488716125\n",
      "Step 1095: val loss: 0.21774820983409882\n",
      "Step 1096: train loss: 0.24291351437568665\n",
      "Step 1096: val loss: 0.2177475094795227\n",
      "Step 1097: train loss: 0.2429121434688568\n",
      "Step 1097: val loss: 0.2177465260028839\n",
      "Step 1098: train loss: 0.2429107427597046\n",
      "Step 1098: val loss: 0.21774590015411377\n",
      "Step 1099: train loss: 0.24290931224822998\n",
      "Step 1099: val loss: 0.21774496138095856\n",
      "Step 1100: train loss: 0.24290814995765686\n",
      "Step 1100: val loss: 0.21774423122406006\n",
      "Step 1101: train loss: 0.24290676414966583\n",
      "Step 1101: val loss: 0.2177434116601944\n",
      "Step 1102: train loss: 0.24290549755096436\n",
      "Step 1102: val loss: 0.21774257719516754\n",
      "Step 1103: train loss: 0.2429041862487793\n",
      "Step 1103: val loss: 0.2177419811487198\n",
      "Step 1104: train loss: 0.242902934551239\n",
      "Step 1104: val loss: 0.2177412062883377\n",
      "Step 1105: train loss: 0.24290165305137634\n",
      "Step 1105: val loss: 0.21774044632911682\n",
      "Step 1106: train loss: 0.24290037155151367\n",
      "Step 1106: val loss: 0.21773964166641235\n",
      "Step 1107: train loss: 0.24289914965629578\n",
      "Step 1107: val loss: 0.21773910522460938\n",
      "Step 1108: train loss: 0.24289804697036743\n",
      "Step 1108: val loss: 0.21773836016654968\n",
      "Step 1109: train loss: 0.24289673566818237\n",
      "Step 1109: val loss: 0.2177375853061676\n",
      "Step 1110: train loss: 0.24289558827877045\n",
      "Step 1110: val loss: 0.21773692965507507\n",
      "Step 1111: train loss: 0.24289438128471375\n",
      "Step 1111: val loss: 0.21773624420166016\n",
      "Step 1112: train loss: 0.24289320409297943\n",
      "Step 1112: val loss: 0.2177356779575348\n",
      "Step 1113: train loss: 0.2428920418024063\n",
      "Step 1113: val loss: 0.21773506700992584\n",
      "Step 1114: train loss: 0.24289090931415558\n",
      "Step 1114: val loss: 0.21773433685302734\n",
      "Step 1115: train loss: 0.24288976192474365\n",
      "Step 1115: val loss: 0.21773377060890198\n",
      "Step 1116: train loss: 0.24288862943649292\n",
      "Step 1116: val loss: 0.21773292124271393\n",
      "Step 1117: train loss: 0.24288754165172577\n",
      "Step 1117: val loss: 0.21773257851600647\n",
      "Step 1118: train loss: 0.24288639426231384\n",
      "Step 1118: val loss: 0.21773181855678558\n",
      "Step 1119: train loss: 0.24288536608219147\n",
      "Step 1119: val loss: 0.21773120760917664\n",
      "Step 1120: train loss: 0.2428842931985855\n",
      "Step 1120: val loss: 0.21773071587085724\n",
      "Step 1121: train loss: 0.24288325011730194\n",
      "Step 1121: val loss: 0.21772992610931396\n",
      "Step 1122: train loss: 0.24288219213485718\n",
      "Step 1122: val loss: 0.2177296280860901\n",
      "Step 1123: train loss: 0.24288110435009003\n",
      "Step 1123: val loss: 0.21772894263267517\n",
      "Step 1124: train loss: 0.24288013577461243\n",
      "Step 1124: val loss: 0.21772849559783936\n",
      "Step 1125: train loss: 0.24287913739681244\n",
      "Step 1125: val loss: 0.21772783994674683\n",
      "Step 1126: train loss: 0.24287813901901245\n",
      "Step 1126: val loss: 0.217727392911911\n",
      "Step 1127: train loss: 0.24287718534469604\n",
      "Step 1127: val loss: 0.2177269011735916\n",
      "Step 1128: train loss: 0.24287612736225128\n",
      "Step 1128: val loss: 0.2177264243364334\n",
      "Step 1129: train loss: 0.24287517368793488\n",
      "Step 1129: val loss: 0.21772587299346924\n",
      "Step 1130: train loss: 0.24287419021129608\n",
      "Step 1130: val loss: 0.217725470662117\n",
      "Step 1131: train loss: 0.24287323653697968\n",
      "Step 1131: val loss: 0.2177249938249588\n",
      "Step 1132: train loss: 0.24287229776382446\n",
      "Step 1132: val loss: 0.21772439777851105\n",
      "Step 1133: train loss: 0.24287140369415283\n",
      "Step 1133: val loss: 0.21772409975528717\n",
      "Step 1134: train loss: 0.2428704798221588\n",
      "Step 1134: val loss: 0.21772357821464539\n",
      "Step 1135: train loss: 0.24286949634552002\n",
      "Step 1135: val loss: 0.21772322058677673\n",
      "Step 1136: train loss: 0.24286863207817078\n",
      "Step 1136: val loss: 0.21772274374961853\n",
      "Step 1137: train loss: 0.2428678274154663\n",
      "Step 1137: val loss: 0.21772220730781555\n",
      "Step 1138: train loss: 0.2428668588399887\n",
      "Step 1138: val loss: 0.2177218645811081\n",
      "Step 1139: train loss: 0.24286599457263947\n",
      "Step 1139: val loss: 0.21772147715091705\n",
      "Step 1140: train loss: 0.24286514520645142\n",
      "Step 1140: val loss: 0.21772100031375885\n",
      "Step 1141: train loss: 0.24286435544490814\n",
      "Step 1141: val loss: 0.21772071719169617\n",
      "Step 1142: train loss: 0.2428634762763977\n",
      "Step 1142: val loss: 0.21772009134292603\n",
      "Step 1143: train loss: 0.24286265671253204\n",
      "Step 1143: val loss: 0.21771983802318573\n",
      "Step 1144: train loss: 0.2428618222475052\n",
      "Step 1144: val loss: 0.2177194058895111\n",
      "Step 1145: train loss: 0.24286092817783356\n",
      "Step 1145: val loss: 0.21771912276744843\n",
      "Step 1146: train loss: 0.24286013841629028\n",
      "Step 1146: val loss: 0.2177187204360962\n",
      "Step 1147: train loss: 0.2428593635559082\n",
      "Step 1147: val loss: 0.21771837770938873\n",
      "Step 1148: train loss: 0.24285860359668732\n",
      "Step 1148: val loss: 0.2177180051803589\n",
      "Step 1149: train loss: 0.24285773932933807\n",
      "Step 1149: val loss: 0.21771758794784546\n",
      "Step 1150: train loss: 0.24285699427127838\n",
      "Step 1150: val loss: 0.21771728992462158\n",
      "Step 1151: train loss: 0.2428562492132187\n",
      "Step 1151: val loss: 0.21771696209907532\n",
      "Step 1152: train loss: 0.2428554892539978\n",
      "Step 1152: val loss: 0.2177167385816574\n",
      "Step 1153: train loss: 0.24285484850406647\n",
      "Step 1153: val loss: 0.2177162915468216\n",
      "Step 1154: train loss: 0.242854043841362\n",
      "Step 1154: val loss: 0.21771599352359772\n",
      "Step 1155: train loss: 0.24285335838794708\n",
      "Step 1155: val loss: 0.21771566569805145\n",
      "Step 1156: train loss: 0.2428526133298874\n",
      "Step 1156: val loss: 0.21771538257598877\n",
      "Step 1157: train loss: 0.24285182356834412\n",
      "Step 1157: val loss: 0.2177150696516037\n",
      "Step 1158: train loss: 0.2428511083126068\n",
      "Step 1158: val loss: 0.21771477162837982\n",
      "Step 1159: train loss: 0.24285049736499786\n",
      "Step 1159: val loss: 0.21771444380283356\n",
      "Step 1160: train loss: 0.24284984171390533\n",
      "Step 1160: val loss: 0.21771423518657684\n",
      "Step 1161: train loss: 0.24284900724887848\n",
      "Step 1161: val loss: 0.21771392226219177\n",
      "Step 1162: train loss: 0.24284832179546356\n",
      "Step 1162: val loss: 0.21771357953548431\n",
      "Step 1163: train loss: 0.2428477704524994\n",
      "Step 1163: val loss: 0.2177133709192276\n",
      "Step 1164: train loss: 0.24284705519676208\n",
      "Step 1164: val loss: 0.21771296858787537\n",
      "Step 1165: train loss: 0.24284636974334717\n",
      "Step 1165: val loss: 0.21771275997161865\n",
      "Step 1166: train loss: 0.2428458034992218\n",
      "Step 1166: val loss: 0.21771259605884552\n",
      "Step 1167: train loss: 0.24284514784812927\n",
      "Step 1167: val loss: 0.2177123874425888\n",
      "Step 1168: train loss: 0.2428445816040039\n",
      "Step 1168: val loss: 0.2177121490240097\n",
      "Step 1169: train loss: 0.242843896150589\n",
      "Step 1169: val loss: 0.21771186590194702\n",
      "Step 1170: train loss: 0.24284321069717407\n",
      "Step 1170: val loss: 0.21771149337291718\n",
      "Step 1171: train loss: 0.2428426742553711\n",
      "Step 1171: val loss: 0.21771129965782166\n",
      "Step 1172: train loss: 0.24284204840660095\n",
      "Step 1172: val loss: 0.21771112084388733\n",
      "Step 1173: train loss: 0.24284151196479797\n",
      "Step 1173: val loss: 0.21771085262298584\n",
      "Step 1174: train loss: 0.24284082651138306\n",
      "Step 1174: val loss: 0.21771074831485748\n",
      "Step 1175: train loss: 0.24284030497074127\n",
      "Step 1175: val loss: 0.2177104502916336\n",
      "Step 1176: train loss: 0.24283963441848755\n",
      "Step 1176: val loss: 0.2177102267742157\n",
      "Step 1177: train loss: 0.24283908307552338\n",
      "Step 1177: val loss: 0.2177099585533142\n",
      "Step 1178: train loss: 0.24283847212791443\n",
      "Step 1178: val loss: 0.21770988404750824\n",
      "Step 1179: train loss: 0.24283790588378906\n",
      "Step 1179: val loss: 0.21770954132080078\n",
      "Step 1180: train loss: 0.24283741414546967\n",
      "Step 1180: val loss: 0.21770961582660675\n",
      "Step 1181: train loss: 0.2428368777036667\n",
      "Step 1181: val loss: 0.21770933270454407\n",
      "Step 1182: train loss: 0.24283626675605774\n",
      "Step 1182: val loss: 0.21770921349525452\n",
      "Step 1183: train loss: 0.24283573031425476\n",
      "Step 1183: val loss: 0.21770893037319183\n",
      "Step 1184: train loss: 0.24283526837825775\n",
      "Step 1184: val loss: 0.21770870685577393\n",
      "Step 1185: train loss: 0.24283474683761597\n",
      "Step 1185: val loss: 0.21770860254764557\n",
      "Step 1186: train loss: 0.242834210395813\n",
      "Step 1186: val loss: 0.2177082896232605\n",
      "Step 1187: train loss: 0.2428336888551712\n",
      "Step 1187: val loss: 0.21770831942558289\n",
      "Step 1188: train loss: 0.2428331971168518\n",
      "Step 1188: val loss: 0.21770797669887543\n",
      "Step 1189: train loss: 0.2428327053785324\n",
      "Step 1189: val loss: 0.2177080512046814\n",
      "Step 1190: train loss: 0.24283213913440704\n",
      "Step 1190: val loss: 0.2177078276872635\n",
      "Step 1191: train loss: 0.24283169209957123\n",
      "Step 1191: val loss: 0.2177075892686844\n",
      "Step 1192: train loss: 0.24283115565776825\n",
      "Step 1192: val loss: 0.21770748496055603\n",
      "Step 1193: train loss: 0.24283063411712646\n",
      "Step 1193: val loss: 0.2177072912454605\n",
      "Step 1194: train loss: 0.24283023178577423\n",
      "Step 1194: val loss: 0.21770721673965454\n",
      "Step 1195: train loss: 0.24282972514629364\n",
      "Step 1195: val loss: 0.2177070677280426\n",
      "Step 1196: train loss: 0.24282924830913544\n",
      "Step 1196: val loss: 0.21770690381526947\n",
      "Step 1197: train loss: 0.24282880127429962\n",
      "Step 1197: val loss: 0.21770687401294708\n",
      "Step 1198: train loss: 0.2428283542394638\n",
      "Step 1198: val loss: 0.21770653128623962\n",
      "Step 1199: train loss: 0.242827907204628\n",
      "Step 1199: val loss: 0.21770668029785156\n",
      "Step 1200: train loss: 0.24282746016979218\n",
      "Step 1200: val loss: 0.2177063375711441\n",
      "Step 1201: train loss: 0.24282699823379517\n",
      "Step 1201: val loss: 0.21770624816417694\n",
      "Step 1202: train loss: 0.24282655119895935\n",
      "Step 1202: val loss: 0.21770629286766052\n",
      "Step 1203: train loss: 0.24282613396644592\n",
      "Step 1203: val loss: 0.21770592033863068\n",
      "Step 1204: train loss: 0.24282565712928772\n",
      "Step 1204: val loss: 0.2177061140537262\n",
      "Step 1205: train loss: 0.24282531440258026\n",
      "Step 1205: val loss: 0.21770572662353516\n",
      "Step 1206: train loss: 0.24282485246658325\n",
      "Step 1206: val loss: 0.2177058756351471\n",
      "Step 1207: train loss: 0.24282440543174744\n",
      "Step 1207: val loss: 0.21770551800727844\n",
      "Step 1208: train loss: 0.2428240180015564\n",
      "Step 1208: val loss: 0.21770568192005157\n",
      "Step 1209: train loss: 0.24282367527484894\n",
      "Step 1209: val loss: 0.2177053987979889\n",
      "Step 1210: train loss: 0.2428232729434967\n",
      "Step 1210: val loss: 0.21770545840263367\n",
      "Step 1211: train loss: 0.2428227961063385\n",
      "Step 1211: val loss: 0.2177051156759262\n",
      "Step 1212: train loss: 0.24282243847846985\n",
      "Step 1212: val loss: 0.21770533919334412\n",
      "Step 1213: train loss: 0.24282203614711761\n",
      "Step 1213: val loss: 0.21770519018173218\n",
      "Step 1214: train loss: 0.2428216189146042\n",
      "Step 1214: val loss: 0.21770519018173218\n",
      "Step 1215: train loss: 0.24282129108905792\n",
      "Step 1215: val loss: 0.2177049219608307\n",
      "Step 1216: train loss: 0.24282091856002808\n",
      "Step 1216: val loss: 0.2177051603794098\n",
      "Step 1217: train loss: 0.24282048642635345\n",
      "Step 1217: val loss: 0.21770484745502472\n",
      "Step 1218: train loss: 0.24282017350196838\n",
      "Step 1218: val loss: 0.21770481765270233\n",
      "Step 1219: train loss: 0.2428198605775833\n",
      "Step 1219: val loss: 0.21770477294921875\n",
      "Step 1220: train loss: 0.24281945824623108\n",
      "Step 1220: val loss: 0.21770469844341278\n",
      "Step 1221: train loss: 0.24281910061836243\n",
      "Step 1221: val loss: 0.2177046239376068\n",
      "Step 1222: train loss: 0.24281877279281616\n",
      "Step 1222: val loss: 0.2177046835422516\n",
      "Step 1223: train loss: 0.24281837046146393\n",
      "Step 1223: val loss: 0.217704638838768\n",
      "Step 1224: train loss: 0.24281805753707886\n",
      "Step 1224: val loss: 0.21770454943180084\n",
      "Step 1225: train loss: 0.2428177297115326\n",
      "Step 1225: val loss: 0.21770447492599487\n",
      "Step 1226: train loss: 0.2428174465894699\n",
      "Step 1226: val loss: 0.21770447492599487\n",
      "Step 1227: train loss: 0.24281705915927887\n",
      "Step 1227: val loss: 0.21770447492599487\n",
      "Step 1228: train loss: 0.24281665682792664\n",
      "Step 1228: val loss: 0.2177044153213501\n",
      "Step 1229: train loss: 0.24281644821166992\n",
      "Step 1229: val loss: 0.21770425140857697\n",
      "Step 1230: train loss: 0.24281610548496246\n",
      "Step 1230: val loss: 0.2177043855190277\n",
      "Step 1231: train loss: 0.24281570315361023\n",
      "Step 1231: val loss: 0.2177044153213501\n",
      "Step 1232: train loss: 0.24281540513038635\n",
      "Step 1232: val loss: 0.21770425140857697\n",
      "Step 1233: train loss: 0.2428150773048401\n",
      "Step 1233: val loss: 0.21770425140857697\n",
      "Step 1234: train loss: 0.2428148239850998\n",
      "Step 1234: val loss: 0.21770422160625458\n",
      "Step 1235: train loss: 0.24281449615955353\n",
      "Step 1235: val loss: 0.21770420670509338\n",
      "Step 1236: train loss: 0.24281421303749084\n",
      "Step 1236: val loss: 0.21770420670509338\n",
      "Step 1237: train loss: 0.2428138554096222\n",
      "Step 1237: val loss: 0.21770401298999786\n",
      "Step 1238: train loss: 0.2428135871887207\n",
      "Step 1238: val loss: 0.21770428121089935\n",
      "Step 1239: train loss: 0.24281339347362518\n",
      "Step 1239: val loss: 0.21770401298999786\n",
      "Step 1240: train loss: 0.24281303584575653\n",
      "Step 1240: val loss: 0.21770422160625458\n",
      "Step 1241: train loss: 0.24281269311904907\n",
      "Step 1241: val loss: 0.21770399808883667\n",
      "Step 1242: train loss: 0.24281245470046997\n",
      "Step 1242: val loss: 0.21770420670509338\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model weights: Parameter containing:\n",
      "tensor([[-7.8553, -4.0753,  1.9758,  1.0087]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10091995)\n",
    "model_final = regression.poly_regr(X_train,y_train,X_val=X_val,y_val=y_val,learning_rate=0.01,num_steps=1243)\n",
    "plt.plot(model_final[1],label = 'Training')\n",
    "plt.plot(model_final[2],label= 'Validation')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Final model weights:',model_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model_evaluation(x,weights):\n",
    "    weights = weights.detach().numpy()\n",
    "    out = (weights[0][-1]*x**3) + (weights[0][-2]*x**2)+(weights[0][-3]*x)+(weights[0][-4])\n",
    "    return out\n",
    "\n",
    "xx = np.linspace(-3,2,1000)\n",
    "y_true=regression.p(xx)\n",
    "y_estim = model_evaluation(x=xx,weights = model_final[0])\n",
    "plt.plot(xx,y_true,label = 'True')\n",
    "plt.plot(xx,y_estim,label='Estimated')\n",
    "#plt.plot(X_train[:,1],y_train,'.')\n",
    "plt.title('True model and estimtated model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d46e833fbb150be313e23f92b66840c9ee391f86f43a5370bfb6957e259579a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
